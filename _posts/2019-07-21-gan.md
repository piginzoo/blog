---
layout: post
title: GAN
category: machine-learning
---

#概述

D判别网络和G生成网络，是2个网络，但是给他们接到一起。他们的训练也是分开的，先训练D（这个时候，要固定住G的参数们），让他有足够强的识别生成图片和真实图片的分辨能力，训练好了D，这个时候，G才登场，他尽量训练自己（D在这个时候参数要固定住），可以骗过D，让D觉得他是真实图片，而不是生成图片。这2个过程，也就是先训练D，后训练G，不停的来，到一定阶段，D和G，就训练好了，这个时候的G，就是我们要的一个需要的生成网络。

有一些问题需要澄清：
- G的输入是啥？答案是啥都行，比如高斯分布的初始化数据，或者随机初始化的等等。初始化的数据的维度也是无所谓的。别太少就成。
- 每次训练D和G到什么程度？一般是固定一个固定的次数，训练这么多次，就切换到另外一个，这个次数是一个超参


#基本原理

恩，推倒 8)

假设生成分布是$P_G(x;\theta)$，真实分布是$P_{data}(x)$

从中真实分布$P_{data}(x)$中采用${x^1,x^2,...,x^m}$，

这是俩分布哈。

那么，我把真实样本（${x^1,x^2,...,x^m}$）带入到生成分布$P_G(x;\theta)$当中，可以求个似然概率吧。

$L=\prod_{i=1}^m P_G(x^i;\theta)$

那么生成分布是要是尽量靠近真实分布的话，那这个似然概率$L$也应该最大吧。然后找出这个使之最大的$\theta$就可以了。

>*不过，这事真是没太想明白，这里有2个变量，一个是到底是啥分布，第二个是确定分布后的参数，不过，如果是是GAN，您不是默认就认为真实分布就应该是生成分布了吧，那就相当于俩变数里确定了一个，那剩下的就一个了，就是参数$\theta$了啊。恩。。。。这么想，这个似然概率有点道理....恩，可以可以可以可以。<-----你丫嘚啵嘚啵自言自语啥呢╮(╯▽╰)╭*

继续继续....推倒

$$
\begin{equation}
\begin{aligned}
&\theta = \mathop{\arg\max}_\theta \prod_{i=1}^m P_G(x_i;\theta) \\
&= \mathop{\arg\max}_\theta \log \prod_{i=1}^m P_G(x_i;\theta) \\
&= \mathop{\arg\max}_\theta  \sum_{i=1}^m \log P_G(x_i;\theta)   \label{x_1,x_2,...,x_m来自于真实样本哈}  \\ 
&\approx \mathop{\arg\max}_\theta E_{x\sim P_{data}} [ \log P_G(x;\theta)] \\


\end{aligned}
\end{equation}
$$

# 参考

- [宏毅老师的GAN讲座](https://www.bilibili.com/video/av24011528)，依然逗逼，通俗易懂
- [中科院的一个系列讲座](https://space.bilibili.com/288145541/channel/detail?cid=48325)，说实话讲的一般
- [某培训机构的讲座](https://www.bilibili.com/video/av26994015/)，讲的一般，不过有了胜于无