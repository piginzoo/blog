---
layout: post
title: GAN
category: machine-learning
---

# 概述

D判别网络和G生成网络，是2个网络，但是给他们接到一起。他们的训练也是分开的，先训练D（这个时候，要固定住G的参数们），让他有足够强的识别生成图片和真实图片的分辨能力，训练好了D，这个时候，G才登场，他尽量训练自己（D在这个时候参数要固定住），可以骗过D，让D觉得他是真实图片，而不是生成图片。这2个过程，也就是先训练D，后训练G，不停的来，到一定阶段，D和G，就训练好了，这个时候的G，就是我们要的一个需要的生成网络。

有一些问题需要澄清：
- G的输入是啥？答案是啥都行，比如高斯分布的初始化数据，或者随机初始化的等等。初始化的数据的维度也是无所谓的。别太少就成。
- 每次训练D和G到什么程度？一般是固定一个固定的次数，训练这么多次，就切换到另外一个，这个次数是一个超参


# 基本原理

## 恩，推倒 8)

假设生成分布是$P_G(x;\theta)$，真实分布是$P_{data}(x)$

从中真实分布$P_{data}(x)$中采用${x^1,x^2,...,x^m}$，

这是俩分布哈。

那么，我把真实样本（${x^1,x^2,...,x^m}$）带入到生成分布$P_G(x;\theta)$当中，可以求个似然概率吧。

$L=\prod_{i=1}^m P_G(x^i;\theta)$

那么生成分布是要是尽量靠近真实分布的话，那这个似然概率$L$也应该最大吧。然后找出这个使之最大的$\theta$就可以了。

>*不过，这事真是没太想明白，这里有2个变量，一个是到底是啥分布，第二个是确定分布后的参数，不过，如果是是GAN，您不是默认就认为真实分布就应该是生成分布了吧，那就相当于俩变数里确定了一个，那剩下的就一个了，就是参数$\theta$了啊。恩。。。。这么想，这个似然概率有点道理....恩，可以可以可以可以。<-----你丫嘚啵嘚啵自言自语啥呢╮(╯▽╰)╭*

继续继续....推倒

$\theta = \mathop{\arg\max}\limits_{\theta} \prod_{i=1}^m P_G(x_i;\theta)$
> 把上面的式子抄下来

$=\mathop{\arg\max}\limits_\theta \log \prod_{i=1}^m P_G(x_i;\theta)$
> 取个log，不犯法

$=\mathop{\arg\max}\limits_\theta  \sum_{i=1}^m \log P_G(x_i;\theta)$   
>${x_1,x_2,...,x_m}$来自于真实样本哈，对，你使用真实样本放到了生成的模型里，诡异哈？！我还是那个理解：就是如果咱俩像，那我$P_{data}$的数据代入到你$P_G$里面也应该似然最大

$\approx \mathop{\arg\max}\limits_\theta E_{x\sim P_{data}} [ \log P_G(x;\theta)]$
>我靠，怎么就约定于了，这画风转变也忒突然了点了吧，李宏毅老师也没有解释，其实这个就是一阶矩估计均值近似于期望的意思，来，看个[参考](https://www.cnblogs.com/tangchong/p/3157710.html)

$=\mathop{\arg\max}\limits_\theta \int_x P_{data}(x) log P_G(x;\theta) dx$
> 这个没啥，就是标准的期望的原始定义


$=\mathop{\arg\max}\limits_\theta [ \int_x P_{data}(x) log P_G(x;\theta) dx - \int_x P_{data}(x) log P_{data}(x;\theta) dx ]$
> "接下来，加一项没有什么用的东西"，宏毅老师如是说，确实是，这个骚操作确实对argmax这个求最大没啥影响，因为后一项和参数$\theta$确实没啥鸟关系，但是总是觉得怪怪的，可能也是Ian Goodfollow这种大神的神来之笔吧，丫这么骚操作，就是为了凑KL散度，那你又要问了，为何要凑KL散度，没啥，就是想说明，你从极大似然，居然可以推出一个KL散度，而已。而已？！恩，至少我是这么理解的。

$=\mathop{\arg\max}\limits_\theta \int_x P_{data}(x) log \frac{P_G(x;\theta)}{P_{data}(x;\theta)}dx$

$=- \mathop{\arg\max}\limits_\theta \int_x P_{data}(x) log \frac{P_{data}(x;\theta)}{P_G(x;\theta)}dx$

$=  \mathop{\arg\min}\limits_\theta \int_x P_{data}(x) log \frac{P_{data}(x;\theta)}{P_G(x;\theta)}dx$
> 整理一下，我去，这个就是KL散度啊，恩，去复习一下[交叉熵、散度](http://www.piginzoo.com/machine-learning/2018/01/03/cross-entropy-kl)概念们

$=\mathop{\arg\min}\limits_\theta KL(P_{data}\\|P_G)$
> 结论是啥，我骚推倒一通后，就是想得到一个结论：其实，你呀忙了半天，就是想找一个$P_G$，让他和$P_{data}$越接近越好（KL散度尽量小），你是不是觉得好有道理啊。不够，瞬间，你是不是有一种被忽悠的感觉，“这不是废话么？！我就是要找一个跟真实分布越相近越好的生成模型嘛”，哈哈。对，你反应过来了，Ian Goodfollow，用一堆数学来证明了，这事是可行的而已。

## 问题来了，咋个找这个$P_G$

如果这个$P_G$是个具体的分布，而且和真实分布$P_{data}$一致，也就是我们知道他的密度函数，那就好办了，我们就用极大似然，求出对应的参数，这事就结了。

但是，残酷的事实是，你不知道$P_{data}$长什么样子，那么，你也不知道自己该如何准备怎样的一个$P_G$，那怎么办呢？

那这样，我们不妨，先用一个深度网络来表示$P_G$，恩，神奇的深度网络，理论上可以拟合任意的函数，也即是可以模拟任意的分布，是一个泛函的空间吧，你可以理解，然后我

# 参考

- [宏毅老师的GAN讲座](https://www.bilibili.com/video/av24011528)，依然逗逼，通俗易懂
- [中科院的一个系列讲座](https://space.bilibili.com/288145541/channel/detail?cid=48325)，说实话讲的一般
- [某培训机构的讲座](https://www.bilibili.com/video/av26994015/)，讲的一般，不过有了胜于无
- [《生成模型---生成对抗网络》- 百度云盘](https://pan.baidu.com/s/1Btp0K5OshPTn75rIhHoiFg) 提取码: 8gqn,一位大神写的小书
- [一个很全的GAN教程](https://echenshe.com/share/dls/)，gitbook的，很方便在线学习
- [GAN的资料整理](https://json0071.gitbooks.io/deeplearning/gan.html)，一个网友整理的，很丰富
- [GAN 论文合集](https://zning.me/15606648118087.html)，看上去很多屌，不过估计很少有人都去读，权当当个检索索引吧，万一需要看呢。
- [万字综述之生成对抗网络 PDF](https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf)，[知乎在线阅读版](https://zhuanlan.zhihu.com/p/58812258),小哥阅读一个GAN综述文献后的总结，良心之作
- PPT们：
