---
layout: post
title: GAN
category: machine-learning
---

# 概述

D判别网络和G生成网络，是2个网络，但是给他们接到一起。他们的训练也是分开的，先训练D（这个时候，要固定住G的参数们），让他有足够强的识别生成图片和真实图片的分辨能力，训练好了D，这个时候，G才登场，他尽量训练自己（D在这个时候参数要固定住），可以骗过D，让D觉得他是真实图片，而不是生成图片。这2个过程，也就是先训练D，后训练G，不停的来，到一定阶段，D和G，就训练好了，这个时候的G，就是我们要的一个需要的生成网络。

有一些问题需要澄清：
- G的输入是啥？答案是啥都行，比如高斯分布的初始化数据，或者随机初始化的等等。初始化的数据的维度也是无所谓的。别太少就成。
- 每次训练D和G到什么程度？一般是固定一个固定的次数，训练这么多次，就切换到另外一个，这个次数是一个超参


# 基本原理

## 恩，推倒 8)

假设生成分布是$P_G(x;\theta)$，真实分布是$P_{data}(x)$

从中真实分布$P_{data}(x)$中采用${x^1,x^2,...,x^m}$，

这是俩分布哈。

那么，我把真实样本（${x^1,x^2,...,x^m}$）带入到生成分布$P_G(x;\theta)$当中，可以求个似然概率吧。

$L=\prod_{i=1}^m P_G(x^i;\theta)$

那么生成分布是要是尽量靠近真实分布的话，那这个似然概率$L$也应该最大吧。然后找出这个使之最大的$\theta$就可以了。

>*不过，这事真是没太想明白，这里有2个变量，一个是到底是啥分布，第二个是确定分布后的参数，不过，如果是是GAN，您不是默认就认为真实分布就应该是生成分布了吧，那就相当于俩变数里确定了一个，那剩下的就一个了，就是参数$\theta$了啊。恩。。。。这么想，这个似然概率有点道理....恩，可以可以可以可以。<-----你丫嘚啵嘚啵自言自语啥呢╮(╯▽╰)╭*

继续继续....推倒

$\theta = \mathop{\arg\max}\limits_{\theta} \prod_{i=1}^m P_G(x_i;\theta)$
> 把上面的式子抄下来

$=\mathop{\arg\max}\limits_\theta \log \prod_{i=1}^m P_G(x_i;\theta)$
> 取个log，不犯法

$=\mathop{\arg\max}\limits_\theta  \sum_{i=1}^m \log P_G(x_i;\theta)$   
>${x_1,x_2,...,x_m}$来自于真实样本哈，对，你使用真实样本放到了生成的模型里，诡异哈？！我还是那个理解：就是如果咱俩像，那我$P_{data}$的数据代入到你$P_G$里面也应该似然最大

$\approx \mathop{\arg\max}\limits_\theta E_{x\sim P_{data}} [ \log P_G(x;\theta)]$
>我靠，怎么就约定于了，这画风转变也忒突然了点了吧，李宏毅老师也没有解释，其实这个就是一阶矩估计均值近似于期望的意思，来，看个[参考](https://www.cnblogs.com/tangchong/p/3157710.html)

$=\mathop{\arg\max}\limits_\theta \int_x P_{data}(x) log P_G(x;\theta) dx$
> 这个没啥，就是标准的期望的原始定义


$=\mathop{\arg\max}\limits_\theta [ \int_x P_{data}(x) log P_G(x;\theta) dx - \int_x P_{data}(x) log P_{data}(x;\theta) dx ]$
> "接下来，加一项没有什么用的东西"，宏毅老师如是说，确实是，这个骚操作确实对argmax这个求最大没啥影响，因为后一项和参数$\theta$确实没啥鸟关系，但是总是觉得怪怪的，可能也是Ian Goodfollow这种大神的神来之笔吧，丫这么骚操作，就是为了凑KL散度，那你又要问了，为何要凑KL散度，没啥，就是想说明，你从极大似然，居然可以推出一个KL散度，而已。而已？！恩，至少我是这么理解的。

$=\mathop{\arg\max}\limits_\theta \int_x P_{data}(x) log \frac{P_G(x;\theta)}{P_{data}(x;\theta)}dx$

$=- \mathop{\arg\max}\limits_\theta \int_x P_{data}(x) log \frac{P_{data}(x;\theta)}{P_G(x;\theta)}dx$

$=  \mathop{\arg\min}\limits_\theta \int_x P_{data}(x) log \frac{P_{data}(x;\theta)}{P_G(x;\theta)}dx$
> 整理一下，我去，这个就是KL散度啊，恩，去复习一下[交叉熵、散度](http://www.piginzoo.com/machine-learning/2018/01/03/cross-entropy-kl)概念们

$=\mathop{\arg\min}\limits_\theta KL(P_{data}\\|P_G)$
> 结论是啥，我骚推倒一通后，就是想得到一个结论：其实，你呀忙了半天，就是想找一个$P_G$，让他和$P_{data}$越接近越好（KL散度尽量小），你是不是觉得好有道理啊。不够，瞬间，你是不是有一种被忽悠的感觉，“这不是废话么？！我就是要找一个跟真实分布越相近越好的生成模型嘛”，哈哈。对，你反应过来了，Ian Goodfollow，用一堆数学来证明了，这事是可行的而已。

## 问题来了，咋个找这个$P_G$

如果这个$P_G$是个具体的分布，而且和真实分布$P_{data}$一致，也就是我们知道他的密度函数，那就好办了，我们就用极大似然，求出对应的参数，这事就结了。

但是，残酷的事实是，你不知道$P_{data}$长什么样子，那么，你也不知道自己该如何准备怎样的一个$P_G$，那怎么办呢？即时我搞出来一个$P_G$，我怎么就能做到让这个$P_G$就和真是分布$P_{data}$很接近呢？en，就是这2个问题：
- $P_G$应该搞成啥样？
- 搞出来的$P_G$到底多像真实分布$P_{data}$呀

那来吧，我们来尝试解决它：
- 用深度网络来表示$P_G$，恩，神奇的深度网络，理论上可以拟合任意的函数，也即是可以模拟任意的分布，是一个泛函的空间吧，你可以理解。
- 把$P_G$和$P_{data}$灌给一个分类器，让他来分辨差异性，如果分表不出来的时候，就说明$P_G$非常非常像$P_{data}$了，对，灌给一个分类器D，2分类，灌进去的是生成的样本和真实的样本，分类结果是你们有多像？0是不像，1是完全一样。

看这段话，我们可以得出：你只要造一个神迹网络（模拟$P_G$），再采样来一堆的$P_G$生成样本，和一堆的$P_{data}$真实样本，这事貌似就可以干了。

好，那我们把这问题，形式化表达一下：

$G^\*= \mathop{\arg\min}\limits_\theta Divergence(P_{data},P_G)$

对，我们就是要找到这样一个$G^\*$，为了找到这个最大的G，我们就需要**引入一个判别器 D**，就是上面提到的那个2分类器（像还是不像的分类器），最终的输出是一个sigmod的分类器就可以了，对，不过它其实也是个神经网络，最不过最后的输出是一个2分类（像，还是不像），然后这就变成了一个优化问题，那还是老规矩，交给“梯度下降”大法来搞定了。

>这个时候，我们需要停顿一下，回魂一下。
思考问题的时候，要保持清醒，清醒的知道：我是谁？我从哪里来？我要去往哪里？
那，回到我们这里，我们在干嘛呢？我们是要找一个生成模型$P_G$去尽可能的靠近真实数据$P_{data}$，那么为了做这事，我引入了一个神经网络**$G$**来表示$P_G$，然后我又需要一个神经网络**$D$**来判断我$P_G$是不是越来越靠近真实分布$P_{data}$了。恩，对，就是这么一个事。

![](/images/20190825/1566708973251.jpg){:class="myimg"}

## 谈谈判别器D

对，我们就是要构建一个判别器，可以尽可能的分别$P_G$生成的样本，和真实样本$P_{data}$，那我们来单独先看看这个判别器，怎么能做好一个判别器，有个好的判别器，就可以逼着生成器不断改进了。

![](/images/20190825/1566714645330.jpg){:class="myimg"}

这个判别器，是一个神经网络。既然是神经网络，就得整个损失函数出来吧，这样就可以做梯度下降，做最优化了呀。损失函数来了：

$V(G,D)= E_{x\sim P_{data}} [ \log P_G(x;\theta)] + E_{x\sim P_G} [ \log (1 - D(x))] $

这个式子咋来了？不知道。不过，直观上还是挺好理解的：**如果来源$x\sim 𝑃_{𝑑𝑎𝑡𝑎}$，$D(x)$尽可能高；如果来源$x \sim 𝑃_𝐺$，$D(x)$尽可能低**。

我们的目标呢，就是：

$ \arg \max \limits_D V(D,G)$

对，去找一个<span class="red">牛逼的D</span>，让这个D可以做到，使得$V(D,G)$最大，这里G假设是固定的了，是之前的已经稳定下来的一个生成器，我们的$D$可以最大化，使得我们最能分辨是生成式数据还是真实数据。

来，让我们更详细地分析分析这个式子：

$V(G,D)= E_{x\sim P_{data}} [ \log D(x;\theta)] + E_{x\sim P_G} [ \log (1 - D(x))] $

$= \int_x P_{data} [ \log D(x)] dx + \int_x P_G [ \log (1 - D(x))] dx $

$= \int_x[P_{data} \log D(x)] + \int_x P_G [ \log (1 - D(x))]dx $

这几步没啥，我们就是要让这个式子最大化，接下来要变态了：

宏毅老师说，为了$max V(G,D)$，G现在已经固定下来了，这个时候就是要找一个$D^\*$,使得$ \arg \max \limits_D V(D,G)$，那咋玩呢？恩，这么玩：

我让上面的积分式中的每一项都最大，宏毅老师的说法是，给定某一个$x$，我觉得都是一个意思：也就是让积分项中的$P_{data} \log D(x)] + \int_x P_G [ \log (1 - D(x))$最大，这个项中，假设某一个x确定，那么变得就是一个$D$，$D$是啥，是一个函数，是一个密度函数，这事貌似就变成了一个泛函问题了，就是找到这么一个$D^\*$，让这项最大化。

>我理解，宏毅老师想表达的是，积分项里面，对每一个$x$，都可以选择一个$D$，让这项最大，那么积分起来，也就是每个$x$的项都加起来，也一定是最大，也就是满足了 $ \arg \max \limits_D V(D,G)$。可是，我总是觉得有些追追不安，你$x1$可以找到一个$D_1$可以使得这个式子最大，而$x2$又可以找到一个$D_2$，可是谁说$D_1,D_2$就一样呢，你没法让两者，甚至更多者统一成一个$D$呀。你这么推导，是不是，牵强呢。有时间的话，得去找Ian Goodfellow的论文看看，看看他怎么“自圆其说”的。

如果按照上面的思路继续往下，其实，就简单了：

就是优化这个式子：$P_{data} \log D(x)] + \int_x P_G [ \log (1 - D(x))$，寻找一个最优的$D$，称之为$D_\*$。这个式子里，变元只有不确定的$D$，嗯，就是个泛函问题了。宏毅老师的推导也简单粗暴：

$P_{data}(x) logD(x) + P_G(x) \log (1 - D(x)$，对应：
- $P_{data}(x)$ => a
- $log D(x)$ => D
- $P_G(x)$ => b

这个式子就变成了：$f(D)=a\*log (D) + b \* log(1-D)$

然后对$D$求偏导，求出最大值$D^\*$:

$\frac{d(f(D))}{dD}= a \* \frac{1}{D} + b \* \frac{1}{1-D} =0$

然后解出来$D^\*$:

$D^\* = \frac{a}{a+b} = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}$

好，既然算出了每项中使之最大的$D^\*$，那么把这个$D^\*$带回到之前的$V(G,D)$中，去求求$V(G,D)$的最大值吧：

$V(G,D)= \int_x[P_{data} \log D(x)] + \int_x P_G [ \log (1 - D(x))]dx $

$= \int_x[P_{data} \log \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + \int_x P_G [ \log (1 - \frac{P_{data}(x)}{P_{data}(x)+P_G(x)})]dx $

$= \int_x[P_{data} \log \frac{1/2 \* P_{data}(x)}{(P_{data}(x)+P_G(x))/2}] + \int_x P_G [ \log (1 - \frac{1/2 \* P_{data}(x)} {(P_{data}(x)+P_G(x))/2})]dx $

$= 
\int_x P_{data} \log * \frac{1}{2} + 
\int_x[P_{data} \log \frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}] + 
\int_x P_G \log * \frac{1}{2} + 
\int_x P_G [ \log (1 - \frac{P_{data}(x)} {(P_{data}(x)+P_G(x))/2})]dx 
$

$= 2log \frac{1}{2} +
\int_x[P_{data} \log \frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}] + 
\int_x P_G [ \log (1 - \frac{P_{data}(x)} {(P_{data}(x)+P_G(x))/2})]dx 
$

$= 2log \frac{1}{2} +
\int_x[P_{data} \log \frac{P_{data}(x)}{(P_{data}(x)+P_G(x))/2}] + 
\int_x P_G [ \log (\frac{P_G(x)} {(P_{data}(x)+P_G(x))/2})]dx 
$

$= 2log \frac{1}{2} +
	KL\left\(	
		P_{data} \\| \frac{P_{data}(x)+P_G(x)}{2}
	\right\) + 
	KL\left\(	
		P_G      \\| \frac{P_{data}(x)+P_G(x)}{2}
	\right\) 
$

$= 2log \frac{1}{2} + 2JSD(P_{data}\\|P_G)$

哈，有点乱！是吧。没事，你自己也推一遍吧。我是老老实实的推了一遍，一点也不难，其实，主要工夫都花在latex的排版上了，很多推导看上去挺复杂，其实就是个机械活动，一步步的推出来就好。

关键还是别迷失，我是谁？我从哪里来？我要去往哪里？恩，我在干嘛呢。必须回顾一下：
>我是在求$V(G,D)$的最大值呢，我让每一项都最大，然后把由一项导出的最大值$D^\*$带回到$V(G,D$，得到了$V^\*(G,D$:

<span class="red">$V^\*(G,D) = 2log \frac{1}{2} + 2JSD(P_{data}\\|P_G)$</span> 

JSD，其实就是[JS散度](https://zxth93.github.io/2017/09/27/KL散度JS散度Wasserstein距离/index.html)。嗯，嗯，你没有看错，最后这个最大值居然就是这个$V(G,D)$最后居然就是JS divergence（JS散度）。[JS散度](https://zxth93.github.io/2017/09/27/KL散度JS散度Wasserstein距离/index.html)是什么鬼来着？其实很简单，就是两个分布的a->b的KL散度和b->a的KL散度的平均。这样避免KL散度有方向性的问题。

**$V(G,D)$是什么来着？估计你都忘了。我提醒你一下，是判别器$D$的最大值，就是说，你这个判别器，尝试去判别生成器生成的样本$P_G$和真实样本$P_{data}$，你的极大值，也就是你最牛逼的判断力，其实就是两个分布的JS散度值，这事说明了什么？什么也不说明，哈哈。只是告诉你一种直觉，冥冥之中自有安排。**

## 回到求解生成器G的上面来

前面提过这个公式：$G^\*= \mathop{\arg\min}\limits_\theta Divergence(P_{data},P_G)$，记得吧，这是啥来着？是生成网络G，孜孜以求的目标。就是要找到这么一个G，可以让G的分布$P_G$和真实数据的$P_{data}$的可衡量的距离，尽量的小。

我们也尝试探讨了，对某个固定下来的G来说，我去寻找一个判别器$D_\*$，可以让$V(G,D$那个损失函数得到最大值，从而让这个D足够牛逼，可以分辨G出来的假东东和真实的数据。而且，这个最大值，其实就是$P_G$和$P_{data}$的JS散度值。

<span class="red">看到了吧，你求解$G_\*$的时候，不是要一个$P_G$和$P_{data}$的Divergence么（差异度）？而那个判别器的算是函数，$V(G,D)$，她的最大值V(G,D^\*)$就是一个Divergence -- JS Divergence呀。 </span>

好啦，把这俩事儿，接到一起，其实就是Ian Goodfellow的GAN论文里的那个著名的GAN的损失函数：

$$\min_{G} \max_{D} V\left(D, G\right) = \mathbb{E}_{\boldsymbol{x} \sim p_{data}{\left(\boldsymbol{x}\right)}}{\left[\log D\left(\boldsymbol{x}\right)\right]} + \mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}\left(\boldsymbol{z}\right)}{\left[\log \left(1 - D\left(G\left(\boldsymbol{z}\right)\right)\right)\right]}$$

也就是，先固定G优化D，使这个式子最大；然后再固定D优化G，使得这个式子达到最小；

![](/images/20190825/1566738436112.jpg){:class="myimg30"}
![](/images/20190825/1566737524448.jpg){:class="myimg30"}

宏毅老师在这块花了不少口舌，他其实就是尝试解释，$\arg\max\limits_D V(G,D)$，就是在衡量$P_G$和$P_{data}$的Divergence，找到了$D^\*$，再去$\arg\min\limits_D V(G,D)$的过程。

左图就是说，你全局假设只有3个G，就从这里面挑一个最好的就成。那你先去看每个G，他对应的$D^\*$得到的$V(G,D^\*)$是多少，这个时候，这个$V(G,D^\*)$就是$P_G$和$P_{data}$的散度距离。然后比较这3个G的$V(G,D^\*)$，就是最好的那个$G^\*$。

右图，把这个过程进一步细化，变成不断迭代的过程，特别解释了在求解$G^\*$的时候，需要优化的式子里面包含一个max，包含max的优化求解有些复杂，需要分段计算梯度值，我理解，大致就是这个细节或者说是难点。

这里，宏毅老师特别强调了一个细节，就是你做梯度下降的时候，你是假设G不是剧烈变化的：

![](/images/20190825/1566738996038.jpg){:class="myimg30"}

如图，左图的蓝色，是上一个迭代的D，你算了他的最大值，也就是$V(G_0,D_0^\*)$，那个最大值（绿色虚线），就是就是$P_G$和$P_{data}$的散度距离 -- JS散度，然后你对G做了一个梯度下降，使之成为了另外一个G:$G_1$（G其实是个泛函了，呵呵，不过用神经网络表示他你也不用太纠结了），别忘了，你的初衷是$G^\*= \mathop{\arg\min}\limits_\theta Divergence(P_{data},P_G)$，如果$G_0$和$G_1$变化剧烈，就像上图一样，那你用的那个D，也就是$D_0^\*$，已经不是$V(G_1,D)$对应的最大值了呀，人家的是$D_1^\*$，不是最大值，这个绿线就不再表示的是JS散度了啊，那完蛋了，这事没的玩了。不过，别担心，宏毅老师的解释是，你做的G的梯度下降，会非常的小的$\delta$变化，这样的话，我们还认为变化后的$G'$，他的最大值还在$D_0^\*$就好。

这样，你就可以放心的更新一遍G的参数了。所以，这也要求你不要太剧烈更新G。实际上，这个细节也深深影响到了后面的训练策略。

总之，宏毅老师废了不少口舌，来讲这个过程。其实，都是在帮着解释Ian Goodfellow的那个著名的GAN损失函数，看着是一个式子，但是其实，他是一个动态的公式，固定一个，训练另外一个，argmax、argmin，不断地交替：（恩，再放一遍这个著名公式）

$$\min_{G} \max_{D} V\left(D, G\right) = \mathbb{E}_{\boldsymbol{x} \sim p_{data}{\left(\boldsymbol{x}\right)}}{\left[\log D\left(\boldsymbol{x}\right)\right]} + \mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}\left(\boldsymbol{z}\right)}{\left[\log \left(1 - D\left(G\left(\boldsymbol{z}\right)\right)\right)\right]}$$

## 好啦，实操训练了



# 参考

- [宏毅老师的GAN讲座](https://www.bilibili.com/video/av24011528)，依然逗逼，通俗易懂。还有这个宏毅老师的[课件](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html)
- [中科院的一个系列讲座](https://space.bilibili.com/288145541/channel/detail?cid=48325)，说实话讲的一般
- [某培训机构的讲座](https://www.bilibili.com/video/av26994015/)，讲的一般，不过有了胜于无
- [《生成模型---生成对抗网络》- 百度云盘](https://pan.baidu.com/s/1Btp0K5OshPTn75rIhHoiFg) 提取码: 8gqn,一位大神写的小书
- [一个很全的GAN教程](https://echenshe.com/share/dls/)，gitbook的，很方便在线学习
- [GAN的资料整理](https://json0071.gitbooks.io/deeplearning/gan.html)，一个网友整理的，很丰富
- [GAN 论文合集](https://zning.me/15606648118087.html)，看上去很多屌，不过估计很少有人都去读，权当当个检索索引吧，万一需要看呢。
- [万字综述之生成对抗网络 PDF](https://github.com/Morde-kaiser/LearningNotes/blob/master/GAN-Overview-Chinese.pdf)，[知乎在线阅读版](https://zhuanlan.zhihu.com/p/58812258),小哥阅读一个GAN综述文献后的总结，良心之作
- PPT们：

过程中，读到的排版又漂亮，内容又干货的小文，忍不住bookmark下：
- https://leovan.me/cn/2018/02/gan-introduction/
- 