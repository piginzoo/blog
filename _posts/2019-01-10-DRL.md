---
layout: post
title: 深度增强学习-DRL
category: machine-learning
---
# 前言
深度增强学习难么？难，也不难。

不难在，没有太多的数学公式，甚至比机器学习那些算法的数学推导都少很多。

难，难在整个体系，概念，忒TM多了。不信？！你就跳到参考资料章节的“概念”里，去看看那一坨概念就明白了。

我的体会是，要搞清深度增强学习，就得先搞清楚增强学习，而增强学习里，马尔科夫决策链，贝尔曼方程，模型和非模型，预测和控制，这些核心概念都得门清。

强化学习的分类：

![](/images/20190114/1547438890285.png){:class="myimg30"}

# 概述
增强学习[reinforcement learning](#)最核心的是学一个策略 $\pi$，然后这个策略$\pi$对某一个环境的状态s，可以给出一个action a，然后a会导致状态发生迁移，并且这个时候，环境会给出一个奖励r。整个过程就是不断地寻找这个$\pi$，找个最好的或者相对好的，从而让奖励的期望值最大。所以，大家都说，增强学习实际上是一个决策算法，帮助你做出更好的决策。



## 马尔科夫决策过程（MDP）

马尔科夫决策过程（Markov decision process,MDP）。它是由⟨$S;A;P;R;\gamma$⟩ 构成的一个五元组，其中：
- S是一个有限状态集，就是你处于哪个状态啊
- A是说采取啥动作
- P 是状态转移概率矩阵：$P^a_{s->s′}$ ，就是你在s，采取a，那么你跳转到下一个状态的s’的转移概率
- R 是一个奖励函数，$R_s^a = E[R_{t+1}\|S_t = s,A_t=a]$，注意，是个期望哈，这个“期望”的概念很重要
- $\gamma$是一个衰减因子: $\gamma\in$[0,1]，这个是为了让整个收益收敛。

可是，你要问了，为何会有转移概率，动作a是如何产生的? 对，好问题。动作a的产生，就是靠一个$\color{red}{策略\pi}$。有了$\pi$，他会告诉你，你当前在这个状态s，会选择哪个动作a，或者以多大概率选择动作a。

是的，总结一下，$P^a_{s->s'}$,$R^a_{s->s'}$，但是某个状态S的时候，应该采取哪个action：a，是由$a=\pi(S)$，也就是策略决定的。

$\color{blue}{画外音}$ 

咱强化学习，学啥啊？就是学这个策略$\pi$啊！！！你还可以把这事用个数学公式表达出来，你丫学的就$a=\pi(s)$，这个太粗暴，啥给个s就得到了个a啊，你可以温柔一点，写成 $\pi(a\|s)$，这就变成了概率分布了，是s给定下的选择某个a的概率分布。恩，这个函数，或者这个概率分布，就是**你学出来的$\pi$啊，这就是我们强化学习的终极目标啊**。

## 收益（$G_t$）

从某一个状态$S_t$开始采样直到终止状态时所有奖励的有衰减的之和:

$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}  ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

恩，注意，$\color{red}{这个值是收敛的}$，这狠重要噢。另外，这里还隐含了一个概念，是说，在t时刻，这家伙处于$S_t$状态呢。

$\color{blue}{画外音}$ 

干嘛攒出来这个收益，还要他收敛呢？是为了衡量咱们的策略$\pi$啊。你想啊，你总的有个量化指标来衡量你的策略牛逼么？为毛就比我的牛逼啊。那好，咱俩都用各自的策略，是骡子是马，来出来溜溜。出来比了，那总的拿个东西衡量吧，这个衡量，就靠这玩意$G_t$，到最后（终止状态），谁收益高，谁的策略$\pi$就牛逼呗。那个$\gamma$是为了让这个值收敛。

BUT，不过，但是，你注意到了么，他的视角不是游戏开头到游戏结束，他是基于某个状态的。我靠，你可能奇怪了，为毛呢？你干嘛从一个状态开始测算收益呢？恩，问的好！是因为，你丫在决策过程中的时候，你玩到半截了，你不是上帝视角，你深陷游戏当中，当局者正迷呢，“兄弟！求你就告诉我，我选a1好，还是选a2，还是选...”，好，我告诉你：你就选择，a之后，从a的那个状态开始，到最后的受益最大的那个a吧。冥冥之中，你就听到我的这个点拨，你就可以毅然决然地选择某个a，杀出重围，继续前行。其实，哪有什么冥冥，这个冥冥就是策略$\pi$啊，“理科男！信概率，别信命！”

不过，你注意到了么？这个是一次的得分噢，你玩吧，玩到哏屁，你的得分。你说“哥们，你这也太武断了把，我就玩一次，你分比我高，就说你的策略比我好，不服！”，对，你说的对，所以，咱们得取个期望，好！引出下面的“价值”，走着，继续看....

## 价值（Value）

对，上面胡喷的时候，有个细节你可能疏漏了。那个收益，只是从某个s开始到哏屁（死了）的一次收益，不行，太随机，所以，要！期！望！

恩，来了

$v(s)= E[G_t\|S_t = s]$

马尔科夫奖励过程中状态收获$G_t$的期望，$G_t$是某个时刻在特定的$S_t$开始，往后一嘟噜决策的收益，对吧？那这玩意随机性很强啊，下次再从这个状态出发，收益可不一定一样啊，肯定是千奇百怪啊，怎么办？取期望啊！所以，就引来了$v(s)$，价值。

$\color{blue}{画外音}$ 


你玩吧，从某个状态s开始哈，玩到死，我给你算个分。然后，你再玩，再玩到死，又得一个分。然后你就不停的玩。玩出期望来。这个期望的分，就是你这个s的$v(s)$。玩无穷次，我靠，这不就是蒙特卡洛采样么？

注意，这个value用于是属于某个s的，来衡量你这个s牛逼不牛逼的。你说了，衡量s牛逼干嘛用呢？兄弟，你知道某个s牛逼了，你选动作a的时候，是不是应该挑选那个大招a'，让他奔着牛逼的s去啊。


## 价值函数（Value Function）

你说了，这个价值怎么算啊？你别告诉我用期望，一想到期望我就头大，得知道概率，得采样，balabala...，不玩了，我玩游戏去了，再见......

回来，兄弟，我有妙招，我给你个函数，你把s传进去，噗呲，这个value就出来了，妙不？

对，这个就是价值函数：

如果存在一个函数，给定一个状态能得到该状态对应的价值，那么该函数就被称为价值函数（value function）。价值函数建立了从状态到价值的映射。

啥意思呢？就是你告诉我一个状态$s$，然后我就告诉你这个期望值$v(s)$，这不就是一个函数么。恩，对，这个函数就是大名鼎鼎的价值函数。

如果状态s是离散的，可数的，你整一个表，来表示这个价值函数就可以。如果丫是连续的，无数个状态s，你咋整？就得用个函数了，甚至用个神经网络来表示了，呵呵，对吧，这里先不细讲，你体会到这个意思就成。

你又该说了，“哥们，你丫别一个劲地忽悠我，你说如果存在一个函数...，这个函数到底到底是啥？”

来，推导一下：

$$
\begin{align}
	& v(s)=E(G_t|S_t=s)  \tag{1}\\
		& =E(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}  ...  |S_t=s)     \\
		& =E(R_{t+1} + \gamma ( R_{t+2} + \gamma R_{t+3}  ...) |S_t=s)     \\
		& =E(R_{t+1} + \gamma G_{t+1}   |S_t=s) \\
		& =E(R_{t+1} + \gamma v(s_{t+1}) |S_t=s) \\
\end{align}
$$

来看看最后这个结论：

$ v(s) = E(R_{t+1} + \gamma \* v(s_{t+1}) \|S_t=s) $

这个式子，说的是，你评估$v(s)$，需要你知道你的下一个可能的迁移过去的$s_{t+1}$的$v(s_{t+1})$，乘以衰减因子$\gamma$，然后再加上系统给你的一个你调到下一个状态时候给的奖励$R_{t+1}$，算出一个期望，才是你s的价值：$v(s)$。

看晕了把，讲人话，就是说，你想评估你当前这个s的价值，得需要你后续的那些$s_{t+1}$的v值们，然后一通骚操作的求个期望，才能评估出来。

![](/images/20190113/1547369463952.png){:class="myimg25"}

注意哈，这里有个细节，就是你评估当前状态的价值value，得用未来s的价值value，哈哈，对，就是这么变态，你问了，当前的值，我都不知道，未来的值我就更不知道了，你这不是逗我玩呢么？！对，我就是逗你玩呢，哈哈。好，不逗你你了，我想你保证，这事是可以干的，一会儿你就知道。

价值函数忒重要了，咱们啊，再换个角度看价值函数

### 换个角度

（其实，不是为了换个啥角度，只是因为，我之前写了一大坨，不舍得删，所以，再让你“换个角度”，再看一遍。哈哈。不过，应该是有点用的，耐心，patient！）

#### 轨迹（τ）

这马尔可夫决策过程，可以类比打游戏。就是给定一个策略 $\pi(a\|s)$，玩一个$\color{red}{轨迹(trajectory)}$，轨迹$τ$也就是你从当前状态开始玩，一直到你哏屁，这个过程叫$τ$, 

$τ = s_0,a_0,s_1,r_1,a_1,··· ,s_{t−1},a_{t−1},s_t ,r_t$，我们来算算这个$τ$的概率：

#### 轨迹概率（$p(τ)$）

$$
	p(τ) = p(s_0, a_0, s_1, a_1, · · · ) \\
	 = p(s_0) \prod_{t=0}^{T-1} \pi(a_t\|s_t) p(s_{t+1}\|s_t,a_t)
$$ 

最后一部分，$p(s_{t+1}\|s_t,a_t)$，其实就是 $\color{red}{游戏本身}$ 呀，你仔细想想，对不对，就是你当前在$s_t$，然后你做了一个action：$a_t$，然后这个概率怎么跳，是你这个agent无法控制的呀。

#### 收益（$G_t$）

恩，在聊一遍$G_t$，跟之前的，上一节讲的，是一个东西。

对于一个$\tau$，也就是一个游戏从头到尾的一个回合来说，这个总的成绩回报是：

对于有限回合和无限回合两种：

![](/images/20190114/1547442125883.png){:width="140px"}
![](/images/20190114/1547442153378.png){:width="100px"}

$\gamma$是折现因子，在[0,1]之间

我们上面知道了轨迹（从某状态到哏屁）概率，知道了收益（某状态开始到死的累计收敛收益），我们想干啥？我们该干啥了？

我们当然是想让这个总和回$G(\tau)$的期望最大啊：

![](/images/20190114/1547442385121.png){:width="250px"}

可是，为何说要求期望最大呢？因为不同的$\tau$这个值是变动的，你这么这么打，下次那么打，所以，用概率来求平均，就变成了期望。

### 上帝视角

[金山-高杨](https://edu.csdn.net/course/play/10684)老师讲的时候，讲到一个上帝视角，挺有启发性的。

看这张图，是游戏的决策，是一个树形结构，这里是极度简化了，这张图就是假设游戏从S1出发就2步游戏就结束。恩，你是上帝，你知道所有的游戏分支，那就好办了，你评估S1的得分问题，就变成了一个树搜索问题了：

![](/images/20190114/1547442779016.png){:class="myimg"}

本质上，算一个$G(\tau)$是一个树搜索问题，算$V(s1)$，就是算s1的时候，最多得分是s1a1s3a1这条路。

这个是插曲哈，是为了让你理解状态的层次的感觉，这段你可以去听高杨老师的讲座，很赞，很快就能帮助理解强化学习的本质。

### 贝尔曼方程

好，开始说说贝尔曼方程了，

前面我们不是说了么，$v(s)$的计算：$ v(s) = E(R_{t+1} + \gamma \* v(s_{t+1}) \|S_t=s) $

好，我们把这个期望按照期望公式展开，即$\sum x\*p(x)$：

$v(s) = R_s + \gamma \* \sum\limits_{s'\in S} P_{ss'} \* v(s') $

恩，这玩意就是贝尔曼方程，看上去不好理解，其实就是个递归方程，要算的$V(S)$其实是对树往下的总得分的一个期望，他没法遍历树，就把$V(S)$表示成它的下一级S'的$V(S')$的$\Sigma$求和。


它提示一个状态的价值 由该状态的奖励以及后续状态价值按概率分布求和按一定的衰减比例联合组成。

牢记：他是用来算一个状态价值的。是一个递归方式定义的。

$\color{blue}{看个例子}$

我给你，跳到某个状态的得分，就是R；再给你转移的概率，就是每条线上的那个概率值；你来算算圈里的值，也就是$v(s)$。

![](/images/20190113/1547369316296.png){:class="myimg"}

这玩意可以看成是一个方程，求解出来$v(s)$，可惜，这玩意求解计算复杂度很高（我也不知道为何？参见叶强的《强化学习》$p_10$页探讨这个问题），那咋办？没事，后续会讲别的办法来求解。

你现在可能有点晕，你朦胧着觉得，$v(s)$就是某个状态的好坏，可以算，用递归方式啥的，可是，你可能很想知道，到底这个值函数能干啥，好，我们来深入探讨一下他都能帮我们干啥：


### 状态价值函数$v(s)$、行为价值函数 $q_\pi(s,a)$

前面咱们讨论了价值函数了，这里再深入讨论他，并且，还要引入一个新的函数行为价值函数，Q函数。

状态价值函数$v(s)$，他其实只描述了一个状态牛逼否，他没有提及动作，可是，我们就是想知道，在某个状态s，我采取某个动作a1，我还可以采取另外一个动作a2，要是能评估从这个s，选哪个动作，可以让我一直玩到哏屁，得分的期望高呢？这个时候，就引入一个新的概念，叫$\color{red}{行为价值函数，q_\pi(s,a)}$。

$q_\pi(s,a) = E(G_t \| S_t = s, A_t = a)$

按照贝尔曼方程展开，

$q_\pi(s,a) = E(R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}) \| S_t = s, A_t = a)$

再展开，

$q_\pi(s,a) = R_{t+1} + \gamma \sum \limits_{s'\in S} P^a_{ss'} v_\pi(s')$

![](/images/20190421/1555850829715.png){:class="myimg25"}

是按照期望展开，就是把所有可能采取的动作action，都按照概率p和对应过去的状态s'的价值，sum到一起，恩，就是期望的公式。对，仔细看，后面是$v(s')$，$q(s,a)$就是“把后续的可能的$v(s')$期望到了一起”！

看上图，看出来了吧，就一半。也就是$v(s)$由$q(s,a1)$和$q(s,a1)$组成，当然不是简单相加，而且按照概率求期望，所以，这就引出了$q(s,a1)$和$v(s)$的关系：

$v_\pi(s) = \sum \limits_{a\in A} \pi(a\|s) q_\pi(s,a)$

然后，我把$q_\pi(s,a)$用上面的式子，带入，得到：

$v_\pi(s) = \sum \limits_{a\in A} \pi(a\|s) \left( R_s^a + \gamma \sum\limits_{s'\in S} P^a_{ss'} v_\pi(s') \right)$

同样的推导，$v(s)$带入到$q_\pi(s,a)$中，得到更详细的$q_\pi(s,a)$:

$q_\pi(s,a) = R_s^a + \gamma \sum \limits_{s\in S} P^a_{ss'} \sum\limits_{a'\in A} \pi(a'\|s') q_\pi(s',a')$

这一通骚操作，都晕了吧，没事，你感兴趣推导，就自己去看叶欣的《强化学习》$p_{12} - p_{13}$去，其实，这一通操作，就是想表达，价值函数和动作函数之间的是$\sum$期望的关系，你捋顺他们的关系，就容易懂了。

### 值函数$v(s)$、行为函数$q(s,a)$的作用

前面说了一堆，回顾一下，啥贝尔曼方程，啥状态价值，啥行为价值。贝尔曼方程，揭示了，当前状态的价值，可以由后续的状态价值递归退出来；从而进一步推导出来，状态价值可以由后续的状态价值求出，也可以由后续的所有的行为价值求出；同理，行为价值又由后续的行为价值求出，也可以由后续的所有的状态价值求出。

恩，然后呢，然后这些关系我搞清楚，我要用学到的这些本事干啥？

求策略啊！！！你丫千万别忘了，我们终极目标，是要找到一个牛逼的策略$\pi$啊。有了$\pi$，我们才可以$a=\pi(s)$，或者由联合概率$\pi(a\|s)$，找到最好的动作action。是滴！行为action，不是石头缝是蹦出来的，而是$\pi$生出来的。


好来，来说说，如何用值函数$v(s)$来评估一个策略牛逼否？

即使假设状态空间 S 和动作空间 A 都是**离散**且**有限**，我们还有个帮助我们决定用哪个动作action的策略$\pi$，策略空间也为$\|A\|^{\|S\|}$，可能会很大。策略$\pi$在这种离散情况下，其实就是一个映射表，从某个s->a。所以策略空间是一个排列组合的关系。（别忘了，是离散+有限情况下）

值函数可以看作是$\color{red}{对策略π的评估}$。如果在状态s，有一个动作a使得$Q_π(s,a) > V_π(s)$，说明执行动作a比当前的策略$π(a\|s)$要好，我们就可以调整参数使得策略 $π(a\|s)$ 的$\color{red}{概率增加}$。

如果我一调整我的策略，让我某状态的状态价值函数$v(s)$变大了，那我就应该坚定不移的调整我的策略，让其达到最优。然后我让所有的状态，每个人，都达到他(状态)的最大状态价值，这时候，我一通骚操作后的策略，就是最优策略了啊！！！

### 最优状态价值函数、最优行为价值函数

所以，引出了“最优状态价值函数、最优行为价值函数”概念：

定义:最优状态价值函数[optimal value function]是所有策略下产生的众多状态价值函数 中的最大者:

$v_*(s) = \max\limits_\pi v_\pi(s)$

定义:最优行为价值函数[optimal action-value function]是所有策略下产生的众多行为价 值函数中的最大者:

$q_*(s,a) = \max\limits_\pi q_\pi(s,a)$

定义:策略$\pi$ 优于 $\pi'(\pi\ge\pi')$，如果对于有限状态集里的任意一个状态s，不等式:$v_{\pi(s)}\ge v_{\pi'}(s)$ 成立。注意！是在$\color{red}{每个状态S}$上，好策略比旧策略的价值函数值$\color{red}都$大！

有个结论：（据说不好证明，只要知道结论就成了）

>对于任何马尔科夫决策过程，存在一个最优策略 $\pi_*$优于或至少不差于所有其它策略。一个马尔科夫决策过程可能存在不止一个最优策略，但最优策略下的状态价值函数均等同于最优状态价值函数:$v^\pi_\* (s) = v_\*(s)$;最优策略下的行为价值函数均等同于最优行为价值函数$q^\pi_\* (s,a) = q_\*(s,a)$。

这话听着好绕口，说的是啥呢？

说的是，可能有多个最优策略，恩，可能有。但是，这些最优策略，最后施加到我们的游戏中，使得最后的每个s的最大状态价值$v_\*(s)$，他们都是一样的。也就是$v_{\pi 1}(s)=v_{\pi 2}(s)=v_\*(s)$

那，这话有啥用呢？

有用，有大用！就是你找到的最大的$v_\*$的时候，那个时刻，对应的就是$v^\pi_\*$，就能找到最好的$\pi$了呀。（\*表示最大）
而$\pi$实际上是对应的$\color{red}{每一个s}$呀，每一个s的$V(s)$都是最大，而，每一个$V(s)$又应该是这个s下对应的所有的action:a，里面最好的那一个。 那么， 你每次只要去找每个状态s下，最好的那个action:a。每个s下都找到了最优的a，那整个最优策略$\pi$不就相当于找到么！大概就是这么一个思路。这叫啥？值迭代？


### 预测和规划

往下，就要开始寻找最佳策略$\pi$之路了，但是，之前，先说俩概念，这俩概念会反复用到：

$\color{red}{预测Prediction：}$

	给你一个马尔科夫决策过程（S，A，P，R，$\gamma$），和一个策略$\pi$，你来算算基于策略的价值函数$v_\pi(s)$，注意，算出来的是每个状态s的$v(s)$，对于离散状态的s，就是个表啊，每个状态对应一个值$v_\pi$。

$\color{red}{控制Control：}$

	给你一个马尔科夫决策过程（S，A，P，R，$\gamma$），你来找出最优的价值函数$v_*(s)$和最优策略$\pi_\*$。

>Q-learning本质是学习最优的Q表，Q表就是最终的策略$\pi$。
怎么学，靠Value Iterator。
理解一个节点的V值
$\forall s, \pi^\* = arg max_\pi V^\pi(s) $
这要是找最优策略$\pi^\*$，挨个枚举$\pi$，然后挨个算一下这个$\pi$下，每个s的值函数$V(s)$,那不得死人啊。

### 策略迭代

从任意一个状态价值函数开始，依据给定的策略，结合贝尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，直至其收敛，得到该策略下最 终的状态价值函数。理解该算法的关键在于在一个迭代周期内如何更新每一个状态的价值。

依据新的策略 π′ 会得到一个新的价值函数，并产生新的贪婪策略，如此重复循环迭代将最 终得到最优价值函数 v∗ 和最优策略 π∗。

从一个初始策略$\pi和初始价值函数$v(s)$开始，基于该策略进行完整的价值评估过程得到一个新的价值函数，随后依据新的价值函数得到新的贪心策略，随后计算新的贪婪策略下的价值函数，整个过程反复进行，在这个循环过程中$\color{red}{策略和价值函数均得到迭代更新}$，并最终收敛值最有价值函数和最优策略。除初始策略外，迭代中的策略均是依据价值函数的贪婪策略。

来源：
<https://blog.csdn.net/trillion_power/article/details/78934608> 
<https://blog.csdn.net/qq_30615903/article/details/80762758>



给一个例子：下图是一个叫Small Gridworld的例子，左上角和右下角是终点，γ=1，移动一步reward减少1，起始的random policy是朝每个能走的方向概率相同，先单独看左边一列，它表示在第k次迭代每个state上value function的值，这一列始终采用了random policy，这里的value function就是通过Bellman Expectation Equation得到的，考虑k=2的情况，-1.7 = -1.0 + 2\*(1/3.0)(-1)，-2.0 = -1.0 + 4(1/4.0)\*(-1)。而右边一列就是在当前的value function情况下通过greedy算法找到当前朝哪个方向走更好。

![](/images/20190115/1547529398323.png){:class="myimg"}



策略评估就是计算任意策略的状态值函数$V_π$，也就是对当前策略下计算出每个状态下的状态值，这就是策略预估，我们也把它称为预测问题。他的计算方式参考之前我们提到过得状态值函数。

$V_\pi(s) = E_\pi[G_t\|S_t=s]$

$V_\pi(s) = E_\pi[R_{t+1}+\gamma G_{t+1}\|S_t=s]$

$V_\pi(s) = E_\pi[R_{t+1}+\gamma V(S_{t+1})\|S_t=s]$

$V_\pi(s) = \sum_a\pi(a\|s)\sum_{s'}P(s'\|s,a)(R(s,a,s′)+γV_\pi(s′))$

因为是策略迭代所以此处的π(a\|s)即为在 s 状态下选取 a 动作的可能，所以策略迭代的公式应该为 

$V_{k+1}(s) = \sum_a\pi(a\|s)\sum_{s',r}P(s',r\|s,a)(r+γV_k(s′))$

k2=2时刻，第一行第二列的1.7怎么来的呢？

他的下个状态分别是坐标<1,1>,<1,3>,<2,2>，三个位置:

$-1.7 = -1.0 + 2\*(1/3.0)(-1)$

参考这个公式：$V_{k+1}(s) = \sum_a\pi(a\|s)\sum_{s',r}P(s',r\|s,a)(r+γV_k(s′))$
- -1.0是k=1时刻<1,2>这个位置的rewords，注意是当前状态的，也就是<1,2>的得分
- 1/3是<1,2>向3个方向转移的概率，即$\pi(a\|s)$
- 这个时候的$P(s'\|s,a)=1$，因为，一个状态经过某个动作（往左、往右、往上、往下）跳到下一个状态是100%确定的
- 3个位置 <1,1>,<1,3>,<2,2>，k=1时刻的，状态值函数的值为0,-1,-1。
- 所以最终的$V_{k=2}(S_{1,2})= 1.7$

这篇的例子足够清晰，建议看一下：

<https://blog.csdn.net/hold_on_me/article/details/81713173>

#### 再次讲解

其实，我承认，上面还是讲的不清楚，我还是再讲一遍吧：

看这个，k=2，s=[1,2]，就是那个“-1”是咋来的？

![](/images/20190422/1555912941337.png){:class="myimg30"}




状态迁移：

```
	self.v_states[state] = reward + self.gamma * self.v_states[next_state]
```



action的选择：

```
	def policy_evaluate(self):
		action = self.states_policy_action[state]
```



**为什么会收敛，参考资料叶强的《 强化学习入门——从原理到实践》p36给出证明**

#### 价值迭代

如果一个 策略不能在当前状态下产生一个最优行为，或者这个策略在针对当前状态的后续状态时不能产生一个最优行为，那么这个策略就不是最优策略。与价值函数对应起来，可以这样描述最优化原则:一个策略能够获得某状态s的最优价值当且仅当该策略也同时获得状态s所有可能的后续状态s′的最优价值。

Q-learning和策略迭代区别？

Q-learning是


## 没有模型

啥叫没有模型？模型是啥？模型就是你知道状态转移概率，以及跳到新状态环境给出的rewards。那么有模型，就是你根本不知道这些，游戏对你来说，就是个黑盒子，你只能靠自己去误打误撞去体验。

>环境已知和模型已知，不是一个概念吧？
答：错，两者就是一回事。参见叶强的RL p49页model-free的定义。环境已知就是status转移概率和reward已知。

那没有模型，你怎么像之前那样，去获得从一个状态开始的累计rewards，也就是$G_t$呢，也进而无法算出$E(G_t)$，也就是算出$V(s)$呀。那咋办呢？

有个办法，你采样！你就从一堆的玩的游戏记录里面，找到你感兴趣的那个状态s，然后，看从这个s开始，又玩了很多步，然后嗝屁的$\tau$，然后大量的找到这样的从s开始到嗝屁的，这个多了，平均一下，不就成了期望了么。


## 参考资料

#### 一些概念
概念实在是太多了，列出来：
- 策略迭代（Policy Iteration）
- 值迭代 （Value Iteration）
- 策略梯度（Policy Gradient）
- 基于模型（Model based）
- 模型无关（Model free）
- 探索与开发（Exploreation-Exploitation）
- 马尔可夫过程（Markov Process）：具备马尔科夫性质的序列$p(s_{t+1}\|s_t, ... , s_0) = p(s_{t+1}\|s_t)$
- 马尔科夫决策过程（MDP - Markov Decision Process）:在马尔可夫过程中 加入一个额外的变量:动作 a,$p(s_{t+1}\|s_t, a_t, · · · , s_0, a_0) = p(s_{t+1}\|s_t, a_t)$
- 确定性策略(Deterministic Policy)：就是张确定的s->a的确定表，当前s，做了某个a，去哪个s'
- 随机性策略(Stochastic Policy):是个概率，s->a->s'，这个s’不是确定的，是个归一化概率
- 轨迹(Trajectory)
- 值函数（Value Function）
- 状态值函数（V\(s\)） - V函数
- 状态-动作值函数（Q\(s,a\)）- Q函数
- 动态规划算法
- 策略评估(policy evaluation)
- 策略改进(policy improvement)
- 蒙特卡罗方法
- 同策略(on policy)
- 异策略(off policy)
- 时序差分学习(temporal-difference learning) - TD
- 贝尔曼方程
- 贝尔曼最优方程
- ε-贪心法
- SARSA算法（State Action Reward State Action，SARSA）
- Q学习(Q-Learning)算法
- 深度Q网络(deep Q-networks，DQN)
- 经验回放(experience replay)
- 目标网络冻结(freezing target networks)
- 策略梯度(policy gradient)
- REINFORCE算法
- 带基准线的 REINFORCE 算法
- Actor-Critic 算法
- A2C算法(Advantage Actor-Critic，A2C)
- A3C算法(Asynchronous Advantage Actor-Critic，A3C)

#### 书系列
- [Sutton老先生的RL 2nd](http://incompleteideas.net/book/RLbook2018trimmed.pdf)，是RL&DRL的泰斗了， David Silver都是他的学生。经典之作，即时是e文的，也得啃啊。
- 周志华老师的西瓜书的第16章:p371 - 强化学习，无链接，买书去哈，这个正版必须支持。
- [邱锡鹏老师的DRL小书-邱老师是FudanNLP作者](https://nndl.github.io/chap-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.pdf)
- [Li Yuxi的一篇综述](https://arxiv.org/abs/1810.06339)
- [知乎网友 叶强自己总结的《 强化学习入门——从原理到实践》](https://pan.baidu.com/s/14Jxp3AGPJFgoFkHa4gXgxA#list/path=%2F)，194页自己的学习笔记，良心之作


#### 视频资料
- B站[国防科技大的沈华伟老师](https://www.bilibili.com/video/av16974313?t=469&p=6)讲《高级人工智能》，就是RL，讲的太好了，遗憾的是第四课木有声音。
- 李宏毅老师讲的[2018深度增强学习](https://www.bilibili.com/video/av24667855?t=1524&p=3)，一如既往的逗逼，不过有点散，可能是上传的人没传全的缘故
- [七月在线的两位老师的课](https://www.bilibili.com/video/av19649554/?p=5)，中规中矩，该讲的都讲了，可以听下去
- [DeepMind亲自出品的教程](https://www.youtube.com/watch?v=iOh7QUZGyiU&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)，等听力过关了要去听一遍。
- [DRL大神 David Silver的培训](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)，还是听力不过关，否则，直接跟着大牛学，多爽啊
- [港科大博士生李思毅：深度强化学习](https://www.bilibili.com/video/av14768722?t=871),这讲座假设你已经了解了，帮你串讲一遍的感觉
- [莫烦的DRL](https://www.bilibili.com/video/av16921335/?p=31)，大家都知道莫老师的系列，初期看，还是很有帮助的
- [万门大学的人工智能、大数据与复杂系统一月特训班](https://www.wanmen.org/courses/593e086f206e46163b6dd5c8)，里面有专门的DRL章节，当然要付费，不过某宝上可以找到，你懂得
- [集智学园的系列课程](http://campus.swarma.org/catalog=37)，集智上的张江老师讲的DRL，还有史雪松、高飞讲的，估计都是张江老师的学生，反正一堆，可以慢慢看
- [金山-高杨](https://edu.csdn.net/course/play/10684)，讲的挺好的，1小时，很浓缩

##### 别人提供的资源
这些都是别人提供的一些DRL的各类资源汇总：
- [Resources for Deep Reinforcement Learning](https://medium.com/@yuxili/resources-for-deep-reinforcement-learning-a5fdf2dc730f)
- (https://zhuanlan.zhihu.com/p/20885568)
- (https://zhuanlan.zhihu.com/p/25965585)
- (https://blog.csdn.net/songrotek/article/details/50572935)
- (https://blog.csdn.net/hai008007/article/details/80274530)
- (http://dataunion.org/14473.html)
- (https://www.zhihu.com/question/277325426/answer/411907338)
