---
layout: post
title: 单据识别学习笔记
category: machine-learning
---

最近开始着手单据扫描的事情，这篇博客主要记录自己的学习笔记。

# 1.综述


开胃菜：先读读这篇别的大神写的[综述](https://zhuanlan.zhihu.com/p/38655369)。

单据识别，其实就俩事，一个是识别字块，也就是**文本检测**，就是找到字所在的区域；一个是**OCR**，也就是把字块中的字一个个地识别出来。

字块识别（文本检测），比较流行的算法就3个，CTPN、EAST、Seglink，这仨都是咱们国人搞出来的，小自豪一下。这篇[小文](https://www.cnblogs.com/skyfsm/p/9776611.html)写的不错，可以参考读一下。

字块识别（文本检测）是发展于目标检测，目标检测本质就是找到包含物体的框，典型的算法有YOLO，SSD，以及Faster-RCNN，而CTPN可就是从Faster-RCNN发展而来的。而Faster-RCNN，又是从RCNN，Fast-RCNN，Faster-RCNN一脉相承地发展而来。

再说说CRNN，OCR的方法，别的我也没看到有啥更牛逼的方法了。

不得不提一下，我对之前的传统图像处理方法，一窍不通，这篇文章，讲的都是深度学习的方法。之前图像方法的巅峰之作，就是谷歌开源出来的tesseract。

另外，不得不说一下，要不是之前各类VGG，CNN，ResNet等深度图像识别网络的大发展，哪有现在这些文本检测呀，这脉络是图像识别->目标识别->文本识别，文本识别里面最基层干活的还是这些基础网络，俗称backbone啊，一般都用VGG16，虽然不是最好的，但是简单易用，transfer迁移学习过来，妥妥的了。

我理解大致就这些，干活也是指望这点水儿，不断地挖掘理解，可能有片面之处，看官们还多海涵我的粗浅，希望探讨的可以点击[关于](/about.html)，速速与我联络，共同探讨。顺道说，这文章没探讨太多细节，细节都在给的一些参考里面，主要是给会的哥们综合学习用，会的自然可以看懂，不会的看了也不会太懂，其实....，其实是写给自己，怕自己过段时间就把这些玩意忘光了，回头一看迅速可以回忆起来，哈哈。

欢迎转载，但是必须给我一个外链哈。

# 2.前戏，先说说目标检测


字块识别、文本检测、文字块检测，叫啥都成，就要得到包含文字的一个框。

说起这玩意，不得不提提，目标检测，也就是Faster-RCNN等那一坨模型。都有啥，主要是RCNN、Fast-RCNN、Faster-RCNN、SSD、YOLO、YOLO-v3。恩，我知道的有点名气的就这些。

其实牛逼点的就3：

- RCNN家族：RBG[Ross B. Girshick]大神搞的，RCNN -> Fast-RCNN -> Faster-RCNN
- YOLO：大神RBG又搞了，果然是大神啊，速度更快，V3是现在的主流
- SSD：没啥人用了，中科院大神搞的


### RCNN 

RCNN就是文本检测最早的一个特别糙的一个模型，就是大神[Ross B. Girshick]搞的，一口气完成这3个，开启了深度学习目标检测的大门。我等必须崇拜一下。

RCNN本质是啥，讲人话就是，用EdgeBoxs、SelectiveSearch，聚类之类的方法，得到一堆(大约2000个左右)的候选框，就是可能包含对象的框（也叫proposal），然后把这些子图归一化，然后交给一个VGG16抽取特征，然后分别交给俩东西：Bbox回归确定边界，SVM分类器确定是不是物体。速速点击[参考](https://blog.csdn.net/v_JULY_v/article/details/80170182)一探究竟。

__细节：__

SVM训练的图是提前切出来给你你丫训练的，上面说SVM只是应用你训练好的模型应用而已；

那个BBox回归也是，回归的是(x,y,w,h)，告诉你坐标在哪里，宽高调整多少。

```
RCNN算法分为4个步骤 
1） 一张图像生成1K~2K个候选区域 （图像分割算法例如 selective search） 
2）对每个候选区域缩放到固定尺寸，使用深度网络提取特征 （CNN模型训练有两步组成：在 ImageNet上的预训练，在检测数据上的微调） 
3） 特征送入每一类的SVM 分类器，判别是否属于该类 
4） 使用回归器精细修正候选框位置
```


### Fast-RCNN

RCNN忒慢，识别一张需要40多秒，所以，就搞了一个Faster-RCNN。

RCNN慢在2000多个候选图片都要经过VGG16算一遍，SSPNet改进这点，咋改进？因为VGG16，也就是CNN之后，得到的feature map（顺道说，feature map就是指你卷积过N次后，得到的那张图，其实丫已经不是张图了，你还可以当图来理解）。之前的某个proposal（备选框），就会对应feature map中的一个区域，（怎么对应，我其实也想特别想清楚，反正池化后，确实是缩小了，肯定是一块比较小的区域了），这样的话，你做一次卷积前向运算就得了，不用2000块都分别做了，快了吧。

但是有个问题，就是大小不一啊，怎么办？他搞了一个下采样池化，说白了，就是把他采样成一样大(50x50)的模样了。

```
Fast RCNN 的算法流程： 
1）用selective search在一张图片中生成约2000个object proposal，即RoI。 
2）把整幅图像输入到全卷积的网络中（这里也可以缩放图片的scale，得到图像金字塔，将多尺度图像送入卷积网络提取卷积特征） 
3）在最后一个卷积层上对每个ROI求映射关系， 并用一个RoI pooling layer提取一个固定维度的特征向量。 
（这里是借鉴了SPPNet中的 SPP网络层，比SPP网络层简单，只用一个尺度） 
4）继续经过两个全连接层（FC）得到特征向量，RoI feature vector。 
5）特征向量兵分两路，经由各自的全连接层（FC），得到两个输出向量： 
第一个是分类，使用softmax，第二个是每一类的bounding box回归。
```

丫确实快多了，才0.4秒。


### Faster-RCNN

大神继续折腾。

这次改进的是proposal，也就是候选框的备选的改进。把region proposal的步骤换成一个CNN网络（RPN）。

里面有个anchor的概念，就是一个可能的框，9个。大小套着，干嘛用的？就是可能是备选框。

RPN(region proposal network)，2步，VGG16的第五层conv5的featuremap一个区域一定对应原图，feature map中的一个点，给丫套一个9个anchor，就是个不同形状的框，对应原图的更大的区域，这个点怎么选？就是画格，然后选中心。这个格子怎么画，是个超参数。

```
Faster R-CNN
1.对整张图片输进CNN，得到feature map
2.卷积特征输入到RPN，得到候选框的特征信息
3.对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
4.对于属于某一类别的候选框，用回归器进一步调整其位置
```

Tensorflow里面的model zoo里面的object detective就是用的faster-rcnn模型。

### YOLO

好吧，怎么都得提一句YOLO，毕竟也是大神RBG之作，而且现在是目标检测主流，只不过咱们搞单据识别不用而已。

Faster-RCNN都NM faster了，还是比YOLO慢。SSD是中科院搞的，很快也，但是准确率差些，不过用的人不多。YOLO好，不过YOLO的问题是作者都是基于自己的duck啥的框架写的。现在V3很赞了。

### 参考

<https://blog.csdn.net/v_JULY_v/article/details/80170182>

<https://blog.csdn.net/u011974639/article/details/78053203>

<https://www.cnblogs.com/lillylin/p/6207119.html>

对了，我是看july七月寒小阳的深度学习《物体识别》那节课迅速入门的。别问我视频哪里搞来的，我买的，你信不？

机器学习网红Siraj Raval的视频
<https://www.youtube.com/watch?v=4eIBisqx9_g>

[老外写的饿，很细，翻译的，中英对照](https://ai.yanxishe.com/page/TextTranslation/1304)

# 3.CTPN

前戏太长，猪脚终于登场。废话少说，直奔娘家：[论文](https://arxiv.org/pdf/1609.03605.pdf)


![](https://pic3.zhimg.com/80/v2-b29f366f73ac0fba695435770e85809e_hd.jpg){:class="myimg"}

![](/images/20190213/1550040628269.png){:width="300px"}

好，看图说话，核心思路：
- 还是老规矩，上VGG16，抽取图像特征
- 出来的feature map，灌给双向Bi-LSTM，输出的最后的向量灌给全连接网络FC
- 学人家Faster-RCNN的RPN方法，学人家的anchor，不过我的anchor宽度固定，x位置固定，你丫9个，我10个。
- 然后对每个备选的anchor，左边分支用于bounding box regression做回归，右边分支用于Softmax做分类（二分类，你丫包含文字否）
- 完事了，把那些从anchor回归+分类后得到的text proposal串糖葫芦，成一个区域

我说说我的理解（好！讲人话）：

CPTN，就是，

用VGG做了5层的CNN后，感受野的抽象，其实就是把图像的特征给抽取出来了，但是，注意！这个时候，图像的布局没变，还是按照原有的布局，一行行的。只不过由于感受野的缘故，其实更有内涵[段子:]。

这个时候，把Conv5=>B-LSTM后的一行行的特征体现，也就是FeatureMap的每一行，的每一个点，这个点上扩展出10个anchor，然后你问我：这每个anchor是不是包含文字呀，介是个分类问题（2分类呀）；需要大小（只有高）位置（只有y）咋个调整啊，介就是个回归问题。

你丫得到了啥，得到了每一个anchor的是否有字，高度和y，对吧？一串哈，每个点扩展的10个anchor都是被这回归和分类搞了一把。搞成啥了？留下那些包含文字的，这些框框，就是我要的text proposal啊。

最后，我用文本线构造算法连接在一起，串成一串，就得到文本位置啦。

### 代码

我fork[小神的代码](https://github.com/eragonruan/text-detection-ctpn)后，研读，然后[写注释版本](https://github.com/piginzoo/text-detection-ctpn)

### 参考文档


- [这篇写好好详细，好详细，里面有CRNN的链接，荐](https://zhuanlan.zhihu.com/p/34757009)

	![](/images/20190213/1550041476524.png){:width="200px"}

	```
	“关于最后一部分的RPN网络:
	1.左边分支用于bounding box regression。由于fc feature map每个点配备了10个Anchor，同时只回归中心y坐标与高度2个值，所以rpn_bboxp_red有20个channels。
	2.右边分支用于Softmax分类Anchor”
	解释：
	10个Anchor就是10个备选框，那么就得看这10个点对应的备选框是不是准确，这个就是我们要回归预测的，但是宽度不用了，另外x不用了，所以参数就变成了高度调整和y的坐标，2个参数了。训练呢？训练是你之前会切成各个的小框

	"多说一句，我看还有人不停的问Anchor大小为什么对应原图尺度，而不是conv5/fc特征尺度。这是因为Anchor是目标的候选框，经过后续分类+位置修正获得目标在原图尺度的检测框。那么这就要求Anchor必须是对应原图尺度！除此之外，如果Anchor大小对应conv5/fc尺度，那就要求Bounding box regression把很小的框回归到很大，这已经超出Regression小范围修正框的设计目的。"
	解释：
	这块要注意，回顾的是原始图片中的框坐标和高度，标签数据也是这么给出的。

	“获得Anchor后，与Faster R-CNN类似，CTPN会做如下处理：
	1.Softmax判断Anchor中是否包含文本，即选出Softmax score大的正Anchor
	2.Bounding box regression修正包含文本的Anchor的中心y坐标与高度。”
	解释：
	可以想象，每一行的anchor，还原回去就是一个框，他上下，包含的文本高度在哪里？标签中是有这些信息的，对吧？然后你就可以对应做出调整了。另外，输入的其实以工行一行输入的，是原有1/16了，而是有“感受野”，所以最后的feature map中其实还包含了一些周边的信息的，不仅仅是原始图片框那个子区域图像。
	```

- [小哥演示手撸代码](https://www.cnblogs.com/skyfsm/p/10054386.html)，还有丫的[同性交友网站链接](https://github.com/AstarLight/Lets_OCR/tree/master/detector/ctpn)

- [有干货，另带Faster-CRNN的链接](https://blog.csdn.net/qq_23225317/article/details/79567691)

- [小哥列的很全，论文，大神链接和训练数据，比较全](https://www.cnblogs.com/lillylin/p/6893500.html)

- [没太多废话，篇幅不长，竟干货](https://www.cnblogs.com/lillylin/p/6277061.html)


# 4.CRNN

好，说完了，如何找到文字块（文本检测），然后呢，就得把那些块里面的字给我识别出来啊，好吧，CRNN上场。顺道吐槽一下，CRNN，RCNN，靠，真像啊，还都偏偏出现在文字识别领域，开始的时候，真让人崩溃。

奇怪的废话少说，学霸控先回娘家，[论文](https://arxiv.org/pdf/1507.05717.pdf)，当然e文不好的又想充学霸的，可以看这个[中文版](https://blog.csdn.net/Quincuntial/article/details/77679463)。

#### CRNN结构
__CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层__：

![](/images/20190213/1550061752866.png){:class="myimg"}

- 1) 卷积层CNN，从输入图像中提取特征序列；

	在进入网络之前，所有的图像需要缩放到相同的高度。特征图的每列对应于原始图像的一个矩形区域（称为感受野），这些矩形区域与特征图上从左到右的相应列具有相同的顺序。特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。

- 2) 循环层RNN，预测每一帧的标签分布；（一帧是一个字符么？）

	RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。（原来是两个网络一起训练的呀！）两个LSTM，一个向前和一个向后组合到一个双向LSTM中。我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。

- 3) 转录层，将每一帧的预测变为最终的标签序列。

	转录是根据每帧预测找到具有最高概率的标签序列。
	存在两种转录模式，即无词典转录和基于词典的转录。
	我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。按照每帧预测y=y1,...,yTy=y1,...,yT对标签序列ll定义概率，并忽略ll中每个标签所在的位置。因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。（讲人话：1是似然 2是不需要单个字符，一堆字符的似然概率作为目标函数）

#### 上面是摘录的，下面是自己讲人话

先说CNN，输入图像，都给丫拍成32像素高，统一就是好，就是好，就是好（石国鹏的包袱），长度不限，3通道(RGB啊)。那输出呢？都统一成（1，25，512），1您就忽略吧，25是什么鬼？是给后续RNN作为输入序列的长度的（就是RNN有25个时间片输入），就是25，规定，乌龟的腚，甭管您输入图像多长，都25。然后512呢？是RNN输入的一个时间片的X的维度，512维？！yes，对！

然后，你丫就用512维度的X，输入25次(就是25个时间片)给LSTM了。那输出呢？我们都知道，RNN输出先是h，然后h做一个softmax，变成概率分布。恩，就是这样，每一个1/25输出一个字母的概率分布，如果是英文就是26个字母+空格，27个。如果是中文，那就是汉字分布喽，4000来个嘛，卧槽，好大的一个概率分布。

```
对于Recurrent Layers，如果使用常见的Softmax Loss，则每一列输出都需要对应一个字符元素。那么训练时候每张样本图片都需要标记出每个字符在图片中的位置，再通过CNN感受野对齐到Feature map的每一列获取该列输出对应的Label才能进行训练。在实际情况中，标记这种对齐样本非常困难，工作量非常大。另外，由于每张样本的字符数量不同，字体样式不同，字体大小不同，导致每列输出并不一定能与每个字符一一对应。当然这种问题同样存在于语音识别领域。例如有人说话快，有人说话慢，那么如何进行语音帧对齐，是一直以来困扰语音识别的巨大难题。
```
讲人话：一般RNN都是输出是某个汉字的概率，可是从图像到汉字的对齐不好对齐啊，标注也不好标注啊，有的宽有的窄啊，那咋办？有个叫CTC的方法，丫直接预测一嘟噜串。就用它！

### CTC

两篇讲CTC的：
号称吴恩达学生，写的很细，很多细节帮助理解CTC:

<https://mp.weixin.qq.com/s/JYgXxHDH4TSerp9nq3NGbg>

<https://mp.weixin.qq.com/s/KaAQczEoESQ__nBaagKg7w>

### 代码

很蛋疼的把能找到的代码都搜了一遍：

- [TF的 star 358，用的Synth 90k训练的（This dataset consists of 9 million images covering 90k English words），有模型可下载，但是e文的。](https://github.com/MaybeShewill-CV/CRNN_Tensorflow)

- [star 137，老外整的，直接忽略（中文支持他都没试过），5天前更新，](https://github.com/Belval/CRNN)

- [star 155，老外的，TF的，最近刚更新14天前](https://github.com/solivr/tf-crnn) 

- [1140个star，原作者的，貌似是，最后更新2年前，使用PyTorch](https://github.com/bgshih/crnn) 

- [52个star，国人的，TF的，不过貌似很简单，不靠谱，最后更新2年前](https://github.com/AimeeKing/crnn-tensorflow) 

- [2431个star，不过是video的，这个只是个子项目，不考虑了，10个月前更新，TF的](https://github.com/AKSHAYUBHAT/DeepVideoAnalytics/tree/master/docs/experiments/ocr) 

- [698个star，好处是CTPN+CRNN，PyTorch的，2年前更的了，模型没有下载，给的链接失效了，博客
是https://blog.csdn.net/u013293750/article/details/73188934，数据是自己生成150万张，“自动生成差不多150万个样本，测试集1500张左右，测试集全对率62%左右。因为硬件限制，所以样本较少，感觉样本数量应该要几千万甚至上亿，模型才会比较稳定。150万个样本训练也没收敛，还有2.5左右的cost.” “中文识别利用crnn训练英文的网络来训练中文，字符个数5529左右，中文的顺序按照tesseract开源项目复制过来的。”](https://github.com/bear63/sceneReco) 

- [721个star，老外，2年前更，pytorch的，预训练模型：https://pan.baidu.com/s/1pLbeCND，没啥用，忽略](https://github.com/meijieru/crnn.pytorch)

- [国人，很热心，star 162个，pyTorch的，4个月前更](https://github.com/Sierkinhane/crnn_chinese_characters_rec)，<https://blog.csdn.net/Sierkinhane/article/details/82857572>博客，360万中文数据集：<https://pan.baidu.com/s/1ufYbnZAZ1q0AlK7yZ08cvQ>。

- [203个star，TF的，1年前更，“在VGG16模型的基础上，迁移训练0、90、180、270度的文字方向分类模型](https://github.com/jiangxiluning/chinese-ocr) ，训练图片100000张”模型可下载：<https://pan.baidu.com/s/1nwEyxDZ>

 - [789个star，包含了文本检测CTPN和文本识：DenseNet + CTC，注意不是CRNN，8月前更](https://github.com/YCG09/chinese_ocr)，数据集：<https://pan.baidu.com/s/1QkI7kjah8SPHwOQ40rS1Pw> (密码：lu7m)，364万张图片，生成自己的样本可参考SynthText_Chinese_version，TextRecognitionDataGenerator和text_renderer

- [240star，数据集链接: https://pan.baidu.com/s/1jJWfDmm 密码: vh8p (中英数300W+,语料不均衡，英文多)，crnn：vgg + blstm + blstm + ctc；densenet-ocr ：densent + ctc，Keras+TF的，8月前更，](https://github.com/xiaomaxiao/keras_ocr) 

- [32 star，国人，就是知乎上那篇不错诶贴子的手撸版](https://github.com/bai-shang/OCR_TF_CRNN_CTC)，[博文](https://zhuanlan.zhihu.com/p/43534801)

- [还有一个](https://github.com/xiaofengShi/CHINESE-OCR)

- [又搜到额一个项目，428star，是中山大学研究生一枚，给学校做的发票识别。](https://github.com/AstarLight/CPS-OCR-Engine) ,[博客是](https://www.cnblogs.com/skyfsm/)

结论：最后还是选择参考CRNN_Tensorflow，需要研读论文，并把其代码完全理解后，训练自己的模型，训练集使用360万那个，以及150万那个（不知道是不是同一个人写的），然后再看准确率。估计需要1周左右。

### 参考文档

<https://blog.csdn.net/Sierkinhane/article/details/82857572>

<https://blog.csdn.net/u012135425/article/details/83375143>



### 趟雷过程

忠实记录一下自己趟雷过程，过于真实，不忍直视

```
首先，搜到了一个哥们的帖子：https://blog.csdn.net/u013293750/article/details/73188934，他还搞了一个github项目（https://github.com/bear63/sceneReco），他的modelctpn文字检测模型，下载不了郁闷。然后，搜到有个哥们什么写了一个攻略：https://blog.csdn.net/u011956004/article/details/79073282，
不过，我都不太感冒，因为都是caffi的，我想搞个tensorflow的，毕竟这个才是主流：然后我就去谷歌了一把，突然搜到一个帖子说有tensorflow版本，
https://blog.csdn.net/weixin_41579863/article/details/79816830
“调研过文本定位的大多看过caffe版的https://github.com/tianzhi0549/CTPN，一直觉得这个效果比较好，偶然发现TensorFlow版本的ctpn，欣喜同时打算跑一跑，可以在这个基础上做迁移学习了。
https://github.com/eragonruan/text-detection-ctpn”
下来，跑，用的人家的model，

最关键的工资水单不行啊。

暂时，心里有点数了，好吧，现在开始搞CRNN。

看到了这篇：https://shimo.im/docs/oDFzLoOXmYsAypwR，讲的不错，也有代码，不过犹豫了一下，
谷歌了一下“crnn tensorflow”，发现了4个github，其中第一个最高，380 star，就他把。

CRNN也是很多版本，选中了这个：
https://github.com/MaybeShewill-CV/CRNN_Tensorflow.git

run.sh 测试一张图片：
python -m tools.demo_shadownet --image_path data/test_images/test_01.jpg --weights_path model/shadownet/shadownet_2017-10-17-11-47-46.ckpt-199999
报错：
NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:
Key shadow/batch_normalization/beta not found in checkpoint
     [[node save/RestoreV2 (defined at /Users/piginzoo/workspace/opensource/CRNN_Tensorflow/tools/demo_shadownet.py:99)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device="/job:localhost/replica:0/task:0/device:CPU:0"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]

啥毛病呢，感觉是模型文件和代码中的模型不一致。
去看他的issue，这个帖子讨论了：https://github.com/MaybeShewill-CV/CRNN_Tensorflow/issues/186

感觉是tf版本的问题，于是重新搞了一个tensorflow的环境：用virtualenv，结果还报错：
安装的时候就警告过：
“tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.”
运行run.sh报错：
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xb
于是安装numpy==1.13.3，还是报错，如旧:
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xb
ImportError: numpy.core.multiarray failed to import
ImportError: numpy.core.umath failed to import
ImportError: numpy.core.umath failed to import
2019-01-22 15:39:44.323473: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr
run.sh: line 1: 12910 Abort trap: 6           python -m tools.demo_shadownet --image_path data/test_images/test_01.jpg --weights_path model/shadownet/shadownet_2017-10-17-11-47-46.ckpt-199999

靠，又报错是numpy的问题，升级版本后，发现还之前的问题相同，放弃环境的搞法。
然后再去看issue，帖子后面还有个办法，就是用其他branch
这个帖子讨论了：https://github.com/MaybeShewill-CV/CRNN_Tensorflow/issues/186
一个老外建议，用branch“@wdsd641417025 If you want to use the trained model provided by the author (in folder model/shadownet) you need to use the code provided in branch revert-138-patch-2 that is in TF version 1.3”

于是checkout其他branch:  git checkout revert-138-patch-2
好了，
对英文没问题，
但是中文图片全都识别成了英文，
后来才发现，他的char_dict只有500多个，根本不是中文的，只是英文的。
看了他的issue发现，果然是大家都是自己train中文的：https://github.com/MaybeShewill-CV/CRNN_Tensorflow/issues/64
不过在这里面发现了一个，这个哥们的
https://github.com/Sierkinhane/crnn_chinese_characters_rec，以及他的csdn博文：https://blog.csdn.net/Sierkinhane/article/details/82857572
他用的是pytorch，靠，可是不想研究那玩意啊，
不过人家给整了一个360万张的训练集：360万中文数据集：https://pan.baidu.com/s/1ufYbnZAZ1q0AlK7yZ08cvQ，标签：https://pan.baidu.com/s/1jfAKQVjD-SMJSffOwGhh8A 密码:u7bo，
不过，他也没给模型！靠！得自己train了。
```

# 5. 一些文章的心得

中间读了一些文章，把其中的文摘摘录出来，以备自己日后参考。

其实，没啥用，都是初期，不靠谱的时候，自己东看西看的一些摘录，写下来了，又不舍得删，就放这里了，客官要是闲的蛋疼，可以看看。

## 参考

<https://mp.weixin.qq.com/s/ULVRX-FpPRtTugrTHrs4lw>

<https://zhuanlan.zhihu.com/p/38655369>

<https://zhuanlan.zhihu.com/p/34584411>

<https://cloud.tencent.com/developer/article/1030422>

<https://www.jishuwen.com/d/2JVF>

<http://www.mooc.ai/open/course/605>

<https://edu.csdn.net/course/play/10506>

<https://mp.weixin.qq.com/s/jvHwFfcvQQdZYpwK3I1DQg>

其他：


<https://github.com/YCG09/chinese_ocr>

这两个不错：

<http://yangxian10.github.io/>

<https://mp.weixin.qq.com/s/Y7Xpe1DlhGR9XRB7GunGnA> 这个综述写的很好


### 笔记1
<https://mp.weixin.qq.com/s/jvHwFfcvQQdZYpwK3I1DQg>

雷音（OCR技术），阿里云（证件、人脸识别技术），MTEE（实时决策引擎），PAI（模型训练、部署平台）。
很多传统方法可以实现特定模糊类型的检测，比如$\color{red}{Laplacian算子法}$，通过计算二阶微分，然后求方差，根据阈值可以确定图像是否模糊。

传统方法在特征提取及特征表现上存在局限性。

本文改进$\color{red}{MobileNetV2}$的网络结构，实现一种新的$\color{red}{模糊检测算法}$。模糊检测需要特别关注图像细节的差异，因此，先通过$\color{red}{随机切片及HSV颜色空间筛选}$的方法生成样本集合，然后基于OCR识别率指标划分正负样本。

原始MobileNetV2网络包含十七层Bottleneck，模型层数较深，并且每层还进行扩展，在实际训练中，不易收敛且模型较大。通过对原始网络进行裁剪和改进，新的结构仅包含两层卷积、两层池化、两层Bottleneck以及一层全连接，网络更浅更窄，模型参数更少。该模糊检测算法的准确率约93.4%，模型原始大小约2M，而使用原始MobileNetV2训练的模型大小约26M。

传统方法可以解决特定的简单的形变问题，比如对于简单的旋转形变，可以通过Hough Transform先检测直线，然后通过旋转角度进行复原。

基于深度学习的方法，如$\color{red}{FCN，STN，Unet}$等，也被尝试用来处理形变问题。本文结合深度学习语义分割领域的相关知识，针对已有方法的不足设计优化方案，提出一种新的形变复原算法。

本文基于$\color{red}{Dilated Convolution}$优化网络结构，并且通过调整损失函数、$\color{red}{平滑预测值等方法，提出一种新的形变复原算法，提升模型的效果。本文采用$\color{red}{MS-SSIM}$作为算法复原效果的评价指标。

存在至少一成的词识别错误，由于没有$\color{red}{针对领域进行优化和分词}$，无法直接阅读和无人化使用，将识别结果进行领域相关的纠错分词，势在必行。

通过数据合成（根据概率转移矩阵，对字符进行增、删、改等编辑操作），以及迁移优化，训练得到满足目标要
求的模型。目前，图片质量较好时，OCR识别结果与$\color{red}{Ground Truth}$的差错率（编辑距离）为15.91%（若忽略空格：2.91%）；经过本文的纠错分词模型，差错率降到2.24%，词准确率提升到93.56%。

### 笔记2
<https://zhuanlan.zhihu.com/p/38655369>

最近流行的技术解决方案中，是用一个多目标网络直接训练出一个$\color{red}{端到端}$的模型。在训练阶段，该模型的输入是
训练图像及图中文本坐标、文本内容，模型优化目标是输出端$\color{red}{边框坐标预测误差与文本内容预测误差的加权和}$。在服务实施阶段，原始图片流过该模型直接输出预测文本信息。

模型基础：起源于图像分类、检测、语义分割等视觉处理任务的各个$\color{red}{基础网络（backbone network）}$。同时，起源于物体检测、语义分割任务的多个网络框架，也被改造后用于$\color{red}{提升}$图文识别任务中的准确率和执行速度。


#### 1.基础网络：


图文识别任务中充当特征提取模块的基础网络，可以来源于$\color{red}{通用}$场景的图像分类模型。例如，VGGNet，ResNet、InceptionNet、DenseNet、Inside-Outside Net、Se-Net等。也可以来源于特定场景的$\color{red}{专用}$网络模型。例如，擅长提取图像细节特征的FCN网络，擅长做图形矫正的STN网络。

**FCN网络**

全卷积网络（FCN,fully convolutional network）， 是$\color{red}{去除了全连接(fc)层}$的基础网络，最初是用于实现$\color{red}{语义分割}$任务。 FC的优势在于利用反卷积（deconvolution）、上池化（unpooling）等上采样（upsampling）操作，将$\color{red}{特征矩阵}$恢复到$\color{red}{接近原图尺寸，然后对$\color{red}{每一个}$位置上的$\color{red}{像素}$做$\color{red}{类别预测}$，从而能识别出更清晰的$\color{red}{物体边界}$。

基于FCN的检测网络，$\color{red}{不再}$经过候选区域$\color{red}{回归}$出物体边框, 而是根据高分辨率的特征图$\color{red}{直接预测物体边框}$。因为不需要像Faster-RCNN那样在训练前定义好候选框长宽比例，FCN在预测$\color{red}{不规则物体边界}$时更加鲁棒。

由于FCN网络最后一层特征图的像素分辨率较高，而图文识别任务中需要依赖清晰的文字笔画来区分不同字符（特别是汉字），所以FCN网络很适合用来$\color{red}{提取文本特征}$。当FCN被用于图文识别任务时，最后一层特征图中每个像素将被分成文字行（$\color{red}{前景}$）和非文字行（$\color{red}{背景}$）两个类别。

**STN网络**

空间变换网络（STN，Spatial Transformer Networks）的作用是对输入特征图进行$\color{red}{空间位置矫正}$得到输出$\color{red}{特征图}$，这个矫正过程是可以进行$\color{red}{梯度传导}$的，从而能够支持端到端的模型训练。

#### 2.检测网络框架

**Faster RCNN**

作为一个检测网络框架，其目标是寻找紧凑包围被检测$\color{red}{对象的边框}$（BBOX，Bounding Box）。

它在Fast RCNN检测框架基础上引入$\color{red}{区域建议网络}$（RPN，Region Proposal Network），来快速产生与目标物体长宽比例接近的多个候选区域参考框（anchor）；

它通过ROI（Region of Interest） Pooling层为多种尺寸参考框产生出归一化固定尺寸的$\color{red}{区域特征}$；

它利用共享的CNN卷积网络同时向上述RPN网络和ROI Pooling层输入特征映射（Feature Maps），从而减少卷积层参数量和计算量。训练过程中使用到了多目标损失函数，包括RPN网络、ROI Pooling层的边框分类loss和坐标回归loss。通过这些loss的梯度反向传播，能够调节候选框的坐标、并增大它与标注对象边框的重叠度/交并比(IOU，Intersection over Union）。RPN网格生成的候选框初始值有固定位置以及长宽比例。如果候选框初始长宽比例设置得与图像中物体形状差别很大，就很难通过回归找到一个紧凑包围它的边框。

**SSD（Single Shot MultiBox Detector）**

是2016年提出的一种$\color{red}{全卷积目标检测算法}$，截止到目前仍是$\color{red}{主要的目标检测框架}$之一，相比Faster RCNN有着明显的$\color{red}{速度}$优势。

SSD是一种$\color{red}{one stage算法}$，$\color{red}{直接预测}$被检测对象的$\color{red}{边框和得分}$。检测过程中，SSD算法利用多尺度思想进行检测，在不同尺度的特征图(feature maps)上产生与目标物体长宽比例接近的多个默认框(Default boxes)，进行回归与分类。

#### 3.文本检测模型

文本检测模型的目标是从图片中尽可能准确地找出文字所在区域。
视觉领域常规物体检测方法(SSD, YOLO, Faster-RCNN等)直接套用于文字检测任务效果并不理想，原因是：
- 相比于常规物体，文字行长度、长宽比例变化范围很大。
- 文本行是有方向性的。常规物体边框BBox的四元组描述方式信息量不充足。
- 自然场景中某些物体局部图像与字母形状相似，如果不参考图像全局信息将有误报。
- 有些艺术字体使用了弯曲的文本行，而手写字体变化模式也很多。
- 由于丰富的背景图像干扰，手工设计特征在自然场景文本识别任务中不够鲁棒。

近年来出现了各种基于深度学习的技术解决方案。它们从特征提取、区域建议网络(RPN)、多目标协同训练、Loss改进、非极大值抑制（NMS）、半监督学习等角度对常规物体检测方法进行改造，极大提升了自然场景图像中文本检测的准确率。
* CTPN方案中，用BLSTM模块提取字符所在图像上下文特征，以提高文本块识别精度。
* RRPN等方案中，文本框标注采用BBOX +方向角度值的形式，模型中产生出可旋转的文字区域候选框，并在边框回归计算过程中找到待测文本行的倾斜角度。
* DMPNet等方案中，使用四边形（非矩形）标注文本框，来更紧凑的包围文本区域。
* SegLink  将单词切割为更易检测的小文字块，再预测邻近连接将小文字块连成词。
* TextBoxes等方案中，调整了文字区域参考框的长宽比例，并将特征层卷积核调整为长方形，从而更适合检测出细长型的文本行。
* FTSN方案中，作者使用Mask-NMS代替传统BBOX的NMS算法来过滤候选框。
* WordSup方案中，采用半监督学习策略，用单词级标注数据来训练字符级文本检测模型。

**CTPN模型** 

CTPN是目前$\color{red}{流传最广、影响最大}$的开源文本检测模型，可以检测$\color{red}{水平或微斜}$的文本行。文本行可以被看成一个$\color{red}{字符sequence}$，而不是一般物体检测中$\color{red}{单个独立}$的目标。同一文本行上各个字符图像间可以互为$\color{red}{上下文}$，在训练阶段让$\color{red}{检测}$模型学习图像中蕴含的这种$\color{red}{上下文统计规律}$，可以使得预测阶段有效提升文本块预测准确率。CTPN模型的图像预测流程中，前端使用当时流行的VGG16做基础网络来提取各字符的$\color{red}{局部图像特征}$，中间使用$\color{red}{BLSTM层}$提取$\color{red}{字符序列上下文特征}$，然后通过FC全连接层，末端经过预测分支输出各个$\color{red}{文字块的坐标值}$和$\color{red}{分类结果概率值}$。在数据后处理阶段，将合并相邻的小文字块为文本行。

**RRPN模型**

基于旋转区域候选网络（RRPN, Rotation Region Proposal Networks）的方案，将$\color{red}{旋转因素}$并入经典区域候选网络（如Faster RCNN）。这种方案中，一个文本区域的ground truth被表示为具有5元组(x,y,h,w,$\color{red}{θ}$)的旋转边框, 坐标(x,y)表示边框的几何中心, 高度h设定为边框的短边，宽度w为长边，方向是长边的方向。训练时，首先生成含有文本方向角的倾斜候选框，然后在边框回归过程中学习文本方向角。

**FTSN模型**

FTSN（Fused Text Segmentation Networks）模型使用$\color{red}{分割网络}$支持倾斜文本检测。它使用$\color{red}{Resnet-101}$做基础网络，使用了多尺度融合的特征图。标注数据包括文本实例的像素掩码和边框，使用像素预测与边框检测多目标联合训练。

**DMPNet模型**

DMPNet（Deep Matching Prior Network）中，使用四边形（$\color{red}{非矩形}$）来更紧凑地标注文本区域边界，其训练出的模型对$\color{red}{倾斜文本块}$检测效果更好。

**EAST模型**

EAST（Efficient and Accuracy Scene Text detection pipeline）模型中，首先使用$\color{red}{全卷积网络}$（FCN）生成多尺度融合的特征图，然后在此基础上直接进行$\color{red}{像素级的文本块}$预测。该模型中，支持旋转矩形框、任意四边形两种文本区域标注形式。对应于四边形标注，模型执行时会对特征图中每个像素预测其到四个顶点的坐标差值。对应于旋转矩形框标注，模型执行时会对特征图中每个像素预测其到矩形框四边的距离、以及矩形框的方向角。

该模型检测英文单词效果较好、检测中文长文本行效果欠佳，不过，省略了其他模型中常见的区域建议、单词分割、子块合并等步骤，因此该模型的执行速度很快。

**SegLink模型**
SegLink模型的标注数据中，先将每个单词切割为更易检测的有方向的小文字块（segment），然后用$\color{red}{邻近连接}$（link ）将各个小文字块连接成单词。这种方案方便于识别长度变化范围很大的、带方向的单词和文本行，它不会象Faster-RCNN等方案因为候选框长宽比例原因检测不出长文本行。相比于CTPN等文本检测模型，SegLink的图片处理速度快很多。

**PixelLink模型**

自然场景图像中一组文字块经常紧挨在一起，通过语义分割方法很难将它们识别开来，所以PixelLink模型尝试用$\color{red}{实例分割}$方法解决这个问题。该模型的特征提取部分，为$\color{red}{VGG16基础上构建的$\color{red}{FCN网络}$。模型执行流程如下图所示。首先，借助于CNN 模块执行两个像素级预测：一个$\color{red}{文本二分类}$预测，一个$\color{red}{链接二分类}$预测。接着，用正链接去连接邻居正文本像素，得到$\color{red}{文字块实例分割}$结果。然后，由分割结果直接就获得文字块边框， 而且允许生成倾斜边框。上述过程中，省掉了其他模型中常见的边框回归步骤，因此训练收敛速度更快些。训练阶段，使用了平衡策略，使得每个文字块在总LOSS中的权值相同。训练过程中，通过预处理增加了各种方向角度的文字块实例。

**Textboxes/Textboxes++模型**

Textboxes是$\color{red}{基于SSD框架}$的图文检测模型，训练方式是$\color{red}{端到端}$的，运行速度也较快。如下图所示，为了适应文字行细长型的特点，候选框的长宽比增加了1,2,3,5,7,10这样初始值。为了适应文本行细长型特点，特征层也用长条形卷积核代替了其他模型中常见的正方形卷积核。为了防止漏检文本行，还在垂直方向增加了候选框数量。为了检测大小不同的字符块，在多个尺度的特征图上并行预测文本框， 然后对预测结果做NMS过滤。

Textboxes++是Textboxes的$\color{red}{升级版}$本，目的是增加对倾斜文本的支持。为此，将标注数据改为了$\color{red}{旋转矩形框和不规则四边形}$的格式；对候选框的长宽比例、特征图层卷积核的形状都作了相应调整。


#### 4. 文本识别模型


文本识别模型的目标是从已$\color{red}{分割出的文字区域}$中$\color{red}{识别}$出$\color{red}{文本内容}$。

**CRNN模型**

CRNN(Convolutional Recurrent Neural Network）是目前较为流行的图文识别模型，可识别较长的文本序列。它包含CNN特征提取层和BLSTM序列特征提取层，能够进行端到端的联合训练。 

它利用BLSTM和CTC部件学习字符图像中的上下文关系， 从而有效提升文本识别准确率，使得模型更加鲁棒。预测过程中，前端使用标准的CNN网络提取文本图像的特征，利用BLSTM将特征向量进行融合以提取字符序列的上下文特征，然后得到每列特征的概率分布，最后通过转录层(CTC rule)进行预测得到文本序列。

**RARE模型**

RARE（Robust text recognizer with Automatic Rectification）模型在识别变形的图像文本时效果很好。如下图所示，模型预测过程中，输入图像首先要被送到一个空间变换网络中做处理，矫正过的图像然后被送入序列识别网络中得到文本预测结果。

如下图所示，空间变换网络内部包含定位网络、网格生成器、采样器三个部件。经过训练后，它可以根据输入图像的特征图动态地产生空间变换网格，然后采样器根据变换网格核函数从原始图像中采样获得一个矩形的文本图像。RARE中支持一种称为TPS（thin-plate splines）的空间变换，从而能够比较准确地识别透视变换过的文本、以及弯曲的文本.

#### 5. 端到端模型

端到端模型的目标是$\color{red}{一站式}$直接从图片中$\color{red}{定位和识别}$出所有文本内容来。

**FOTS Rotation-Sensitive Regression**

FOTS（Fast Oriented Text Spotting）是图像文本检测与识别同步训练、端到端可学习的网络模型。检测和识别任务共享卷积特征层，既节省了计算时间，也比两阶段训练方式学习到更多图像特征。引入了旋转感兴趣区域（RoIRotate）,可以从卷积特征图中产生出定向的文本区域，从而支持倾斜文本的识别.

**STN-OCR模型**

STN-OCR是集成了了图文检测和识别功能的端到端可学习模型。在它的检测部分嵌入了一个空间变换网络（STN）来对原始输入图像进行仿射（affine）变换。利用这个空间变换网络，可以对检测到的多个文本块分别执行旋转、缩放和倾斜等图形矫正动作，从而在后续文本识别阶段得到更好的识别精度。在训练上STN-OCR属于半监督学习方法，只需要提供文本内容标注，而不要求文本定位信息。作者也提到，如果从头开始训练则网络收敛速度较慢，因此建议渐进地增加训练难度。STN-OCR已经开放了工程源代码和预训练模型。

#### 6. 数据集

**Chinese Text in the Wild(CTW)**

该数据集包含32285张图像，1018402个中文字符(来自于腾讯街景), 包含平面文本，凸起文本，城市文本，农村文本，低亮度文本，远处文本，部分遮挡文本。图像大小2048\*2048，数据集大小为31GB。以(8:1:1)的比例将数据集分为训练集(25887张图像，812872个汉字，测试集(3269张图像，103519个汉字)，验证集(3129张图像，103519个汉字)。

文献链接：<https://arxiv.org/pdf/1803.00085.pdf>

数据集下载地址：<https://ctwdataset.github.io/>


**Reading Chinese Text in the Wild(RCTW-17)**

该数据集包含12263张图像，训练集8034张，测试集4229张，共11.4GB。大部分图像由手机相机拍摄，含有少量的屏幕截图，图像中包含中文文本与少量英文文本。图像分辨率大小不等。

下载地址<http://mclab.eic.hust.edu.cn/icdar2017chinese/dataset.html>

文献：<http://arxiv.org/pdf/1708.09585v2>

**ICPR MWI 2018 挑战赛**

大赛提供20000张图像作为数据集，其中50%作为训练集，50%作为测试集。主要由合成图像，产品描述，网络广告构成。该数据集数据量充分，中英文混合，涵盖数十种字体，字体大小不一，多种版式，背景复杂。文件大小为2GB。

下载地址：<https://tianchi.aliyun.com/competition/information.htm?raceId=231651&_is_login_redirect=true&accounttraceid=595a06c3-7530-4b8a-ad3d-40165e22dbfe>


**Total-Text**

该数据集共1555张图像，11459文本行，包含水平文本，倾斜文本，弯曲文本。文件大小441MB。大部分为英文文本，少量中文文本。训练集：1255张 测试集：300

下载地址：<http://www.cs-chan.com/source/ICDAR2017/totaltext.zip>

文献：<http:// arxiv.org/pdf/1710.10400v>

**Google FSNS(谷歌街景文本数据集)**

该数据集是从谷歌法国街景图片上获得的一百多万张街道名字标志，每一张包含同一街道标志牌的不同视角，图像大小为600\*150，训练集1044868张，验证集16150张，测试集20404张。

下载地址：<http://rrc.cvc.uab.es/?ch=6&com=downloads>

文献：<http:// arxiv.org/pdf/1702.03970v1>

**COCO-TEXT**

该数据集，包括63686幅图像，173589个文本实例，包括手写版和打印版，清晰版和非清晰版。文件大小12.58GB，训练集：43686张，测试集：10000张，验证集：10000张

文献: <http://arxiv.org/pdf/1601.07140v2>

下载地址：<https://vision.cornell.edu/se3/coco-text-2/>

**Synthetic Data for Text Localisation**

在复杂背景下人工合成的自然场景文本数据。包含858750张图像，共7266866个单词实例，28971487个字符，文件大小为41GB。该合成算法，不需要人工标注就可知道文字的label信息和位置信息，可得到大量自然场景文本标注数据。

下载地址：<http://www.robots.ox.ac.uk/~vgg/data/scenetext/>

文献：<http://www.robots.ox.ac.uk/~ankush/textloc.pdf>

Code: <https://github.com/ankush-me/SynthText> 英文版

Code <https://github.com/wang-tf/Chinese_OCR_synthetic_data> 中文版

**Synthetic Word Dataset**

合成文本识别数据集，包含9百万张图像，涵盖了9万个英语单词。文件大小为10GB

下载地址：<http://www.robots.ox.ac.uk/~vgg/data/text/>

**Caffe-ocr中文合成数据**

数据利用中文语料库，通过字体、大小、灰度、模糊、透视、拉伸等变化随机生成，共360万张图片，图像分辨率为280x32，涵盖了汉字、标点、英文、数字共5990个字符。文件大小约为8.6GB

下载地址：<https://pan.baidu.com/s/1dFda6R3>

### 笔记3

<https://zhuanlan.zhihu.com/p/34584411>

在2013年之前，传统算法在OCR领域占主导地位，其标准流程包含文本检测、单字符分割、单字符识别、后处理等步骤。

传统方法中具有代表性的PhotoOCR[1]算法。PhotoOCR是谷歌公司提出的一套完整OCR识别系统，包含文字区域检测、文本行归并、过分割、基于Beam Search的分割区域的组合、基于HOG特征和全连接神经网络的单字符分类、基于ngram方法的识别结果校正。

基于CNN的识别算法，该方法由两部分构成，检测模块采用基于 region proposal 和滑动窗的方法切出词条，识别部分采用 7层CNN对整词分类。此方法另一大贡献是提供了大规模合成数据的方法。标注文字的成本远高于标注人脸、物体等数据，高标注成本限制了OCR数据集规模。因此，合成样本方法的出现，有效缓解了深度网络对于OCR真实标注数据的依赖，极大推动了OCR识别领域的深度算法的发展。CNN方法的出现，最大功能是在特征工程及单字符分类领域替代传统方法，但仍然未能避免传统思路中难度最大的二值化和字符分割问题。在复杂的自然场景、广告场景中，CNN分类方法仍难以满足需要。

#### 腾讯DPPR团队场景文字识别技术

基于联结时序分类(Connectionist Temporal Classification, CTC)训练RNN的算法，在语音识别领域[4]显著超过传统语音识别算法。一些学者尝试把CTC损失函数借鉴到OCR识别中，CRNN [5]就是其中代表性算法。CRNN算法输入100\*32归一化高度的词条图像，基于7层CNN提取特征图，把特征图按列切分（Map-to-Sequence），每一列的512维特征，输入到两层各256单元的双向LSTM进行分类。在训练过程中，通过CTC损失函数的指导，实现字符位置与类标的近似软对齐。

本团队也开始尝试把注意力机制引入到OCR识别模块。注意力机制能够聚焦词条图像特征向量的ROI，在当前时刻实现特征向量与原图字符区域的近似对齐，提升深度网络中的Encoder-Decoder模型的聚焦度与准确率。

### 笔记4

<https://cloud.tencent.com/developer/article/1030422>

![](https://ask.qcloudimg.com/http-save/yehe-1269631/adtgfusgum.jpeg?imageView2/2/w/1620){:class="myimg"}

印刷体识别的主要流程大致分为以下几个部分：图像预处理；版面处理；图像切分；特征提取、匹配及模型训练、匹配；识别后处理。

**预处理**

一般包括灰度化、二值化，倾斜检测与校正，行、字切分，平滑，规范化等等。灰度化处理的主要目的就是滤除这些信息，灰度化的实质其实就是将原本由三维描述的像素点，映射为一维描述的像素点。经过灰度处理的彩色图像还需经过二值化处理将文字与背景进一步分离开，二值化效果的好坏，会直接影响灰度文本图像的识别率。二值化方法大致可以分为局部阈值二值化和整体阈值二值化。目前使用较多的日本学者大津提出的“大津法”

**倾斜校正**

文本图像的倾斜检测方法有许多种，主要可以划分为以下$\color{red}{五类}$:基于投影图的方法，基于Houhg变换的方法，基于交叉相关性的方法，基于Fourier变换的方法和基于最近邻聚类方法。

最简单的基于投影图的方法是将文本图像沿不同方向进行投影。当投影方向和文字行方向一致时，文字行在投影图上的峰值最大，并且投影图存在明显的峰谷，此时的投影方向就是倾斜角度。

Huogh变换也是一种$\color{red}{最常用}$的倾斜检测方法，它是利用Hough变换的特性，将图像中的前景像素映射到极坐标空间，通过统计极坐标空间各点的累加值得到文档图像的倾斜角度。

Fourier变换的方法是利用页面倾角对应于使Fourier空间密度最大的方向角的特性，将文档图像的所有像素点进行Fourier变换。这种方法的计算量非常大，目前很少采用。

基于最近邻聚类方法，取文本图像的某个子区域中字符连通域的中心点作为特征点，利用基线上的点的连续性，计算出对应的文本行的方向角，从而得到整个页面的倾斜角。

**规范化**

为了消除文字点阵位置上的偏差，需要把整个文字点阵图移动到规定的位置上，这个过程被称为位置规范化。

常用的位置规范化操作有两种，一种是基于质心的位置规范化，另一种是基于文字外边框的位置规范化。基于文字外边框的位置规范化需要首先计算文字的外边框，并找出中心，然后把文字中心移动到指定的位置上来。基于质心的位置规范化方法抗干扰能力比基于文字外边框的位置规范化方法要强。

**图像平滑**

文本图像经过平滑处理之后，能够去掉笔划上的孤立白点和笔划外部的孤立黑点，以及笔划边缘的凹凸点，使得笔划边缘变得平滑。

**版面分析**

将文本图像分割为不同部分，并标定各部分属性，如：文本、图像、表格。目前在版面分析方面的工作核心思想都是基于连通域分析法，后衍生出的基于神经网络的版面分析法等也都是以连通域为基础进行的。

连通域是指将图像经过二值化后转为的二值矩阵中任选一个像素点，若包围其的所有像素点中存在相同像素值的像素点则视为两点连通，以此类推，这样的像素点构成的一个集合在图像中所在的区域即一个连通域。根据连通域大小或像素点分布等特征可以将连通域的属性标记出来，用作进一步处理的依据。

**版面理解**

获取文章逻辑结构，包括各区域的逻辑属性、文章的层次关系和阅读顺序。根据版面分析时记载的连通域位置信息，确定连通域归属序列。

**图像切分**
行(列）切分和字切分。经过切分处理后，才能方便对单个文字进行识别处理。

[行、列切分]

由于印刷体文字图像行列间距.、字间距大致相等，且几乎不存在粘连现象，所以可以采用投影法对图像进行切分，得到每列（行）在坐标轴的像素值投影曲线是一个不平滑的曲线，通过高斯平滑后的曲线在每个波谷位置间的区域即为要的一行（列）。

[字切分]

字切分对于不同的文种存在着比较明显的差异，通常意义下，字切分是指将整行或整列文字切分成独立的一个个文字，而实际上根据文种差异，可能还需需要将单个文字进行进一步切分。而因为文种不同，构词法或钩字法也有所不同，所以切分方法的难度差别也是天壤之别。例如将汉字“屋”切分开的难度和将英文“house”切分开的难度差别就很大，因此在识别模式上，也会根据文种特性，设计不同的识别方法。

**特征提取及匹配**

特征提取是从单个字符图像上提取统计特征或结构特征的过程。所提取的特征的稳定性及有效性，决定了识别的性能。对于统计特征的提取，可利用统计模式识别中的特征提取方法，而对结构特征的提取，应根据具体文字所确定的识别基元确定相应的特征提取方法。在相当长的文字识别的研究过程中，是利用人们的经验知识，指导文字特征的提取。例如边缘特征、变换特征、穿透特征、网格特征、特征点特征、方向线素特征等等。

特征匹配是从已有的特征库中找到与待识别文字相似度最高的文字的过程。当待识别文字提取完特征之后，不管使用的是统计特征，还是结构特征，都需要有一个特征库来进行比对，特征库中应包含欲识别字符集中所有文字的特征。特征匹配的方法有很多，比较常用的有:欧式空间的比对法、松弛比对法、动态程序比对法以及HMM(HiddneMarkovModel)法等等。在神经网络出现之前以及之后很长一段时间，在汉字OCR领域，一直采用的就是这种模板匹配的方法。












