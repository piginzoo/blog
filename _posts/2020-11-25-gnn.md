---
layout: post
title: GNN图神经网络
category: machine-learning
---

## 概述

CNN只适合欧式空间，啥叫欧式空间，就是支持[距离公式](https://baike.baidu.com/item/%E4%B8%A4%E7%82%B9%E9%97%B4%E8%B7%9D%E7%A6%BB%E5%85%AC%E5%BC%8F)的空间。
GNN更通用，适合非欧空间。
那么问题来了，我欧式空间（如图像）上，可以做卷积，那我图上，如何做卷积呢？
图神经网络最核心的就是要解决卷积的问题。
卷积最核心的就是卷积核如何确定。
那么问题又来了，什么叫卷积的核？


### 图

先给定一个图的基本概念：

$G=(V,E,W)$

- V 是节点（vertex）集合，其中节点用一个d维向量表示，$n=\|V\|$
- X 是节点的向量矩阵，就列就是一个节点的表示，$X \in R^{n \times d}$
- E 是边（edge）集合
- W 是邻接矩阵（方阵），矩阵里的值，就是边的权重，$W \in R^{n \times n}$

可以把每个节点当做是一个信号（看，和信号处理勾稽上了）。

### 卷积

本质上卷积核，是对在过滤不同频率的信号。[参考](https://zhuanlan.zhihu.com/p/28478034)。
你看卷积公式$h(t) = \int_{+\infty}^{-\infty} f(\tau)g(x-\tau) dt$，

![](/images/20201127/1606448534258.jpg)

[参考](https://www.bilibili.com/video/BV1Ry4y1r73h?from=search&seid=7657366210802615266)：视频22分钟处。

- 对应上图：$f(\tau)$就是右侧那个下降函数，$g(x-\tau)$是方波函数，而卷积结果（是个以时间$\t$为变量的函数）是左侧那个上升函数
- $h(t)$是一个以$t$为变量的函数，是的，他是个时间函数
- 设想某个$t=t_0$，$h(t_0)$就是那上图中黄色面积
- 这个面积是一个积分，是$\tau$从$-\infty ~ +\infty$的积分，上图上，就是黄色的面积

那么问题又来了，卷积有啥用？

其实，卷积本质是为了在频域空间的不同频率信号的过滤，原因是，时域卷积=频域相乘，这个是傅里叶变换的结论，细节我会在傅里叶变换的博文中详解，这里只知道结论就可以。你用一个滤波器函数$g(x)$去和你的目标函数$f(x)$做卷积，就相当于是把他们放到了频率里，进行相乘，相乘的结果，就相当于在频率里把某些频段的值给“乘”掉了（理解这个可以参考下面参考问斩各种的方波滤波器的概念），只剩下了你所需要的频段的信号了。

>时域卷积=频域相乘，卷积核本质上是一个二维函数，有对应的频谱函数，因而可以看成某种『滤波器』

[参考](https://zhuanlan.zhihu.com/p/28478034)

那么，把这个概念扩展到二维函数上，也就是图像上，图像可以看做是一个二维离散函数。
图像上的分类，其实就是图像上的信号处理。
那么我们来对应一下卷积的概念。
$f(x)和g(x)$的x，对应整个图像的二维位置（x，y），$f(x)$对应图像的值，$g(x)$就是对应的卷积核，时间$t$对应的就是卷积目标feature map上的一个点。你的卷积核，一个一个位置的挪动，就是在计算目标函数$h(t)$。看看，都对应上了吧！

可是，我们又要想，为何要做图像的卷积？

答案是，为了提取图像不同频率的信息。啥？！图像又不是波，怎么会有频率？有的！图像实际上可以看成是一个由不同的点上的值变化，所组成的一个东西。不同的点（二维），对应不同的值，这就是一个函数啊。既然是函数，就可以按照傅里叶变换，变换到频率里。这么说的太数学化，不好懂。你可以换个思路理解，就是像素间变化剧烈，比如从红色突然变成了白色，或者从255的值，突然变成了0，这种变化多剧烈啊！这就是高频啊。反之，就是低频啊。

### 图上的卷积

图片上可以做卷积，我们理解了，那么问题又来了？一个图（Graph）上，如何做卷积？


如何做图卷积的卷积核的参数共享？

### 分类


#### 谱方法

图上的拉普拉斯，

$L=D - W$

- W上面提到了，就是边上的权重，是一个对称方阵，$W \in R^{n \times n}$
- D呢？我看很多文章是都说图的节点的度的对角阵，但是实际上D的定义是：$D_{ij}= \sum_j W_{ij}$。

	这里不得不说一下，网上的狠多文章都是不考虑边权重的图，或者说，权重是1，但是如果是边带不同权重的，就需要用上述公式来确定矩阵D，即一个边出度上的边的权重之合。



#### 空间方法






## 典型GNN

### GCN

GCN可谓是图神经网络的“开山之作”，首次将图像卷积操作用到图结构。

$h_v = f(\frac{1}{\|N(v)\|} \sum_{u\in N(v)} Wx_u+b)  ,   \forall v \in V$

聚合邻居节点的特征然后做一个线性变换

https://www.zhihu.com/question/54504471

GCN的缺点:
- GCN需要将整个图放到内存和显存，这将非常耗内存和显存，处理不了大图；
- GCN在训练时需要知道整个图的结构信息(包括待预测的节点)

### GraphSAGE

https://zhuanlan.zhihu.com/p/136521625

Graph Sample and Aggregate

GCN输入了整个图，无法区分训练和验证集。

GraphSAGE对k-hop（距离为k的节点们）进行采样，对这些采出来的节点上的embeding值进行聚合，

【咋采样？】

采用一个叫“定长抽样”的方法，定义需要的邻居个数 [公式] ，然后采用有放回的重采样/负采样方法达到 [公式] 。保证每个节点（采样后的）邻居个数一致？？？

【咋聚合？参数呢？】

就是如何根据周边的节点，算出当前目标节点的更新后的embeding值。(你看，其实图结构没变)

【损失函数】

如果是有监督的情况下，可以使用每个节点的预测lable和真实lable的交叉熵作为损失函数。如果是在无监督的情况下，可以假设相邻的节点的embedding表示尽可能相近。


### GAT

![](/images/20201127/1606460968299.jpg){:class="myimg"}

## 实现

## 开源项目

## 参考

### 文档
- [GNN 入门系列: 直观理解和应用介绍](https://mp.weixin.qq.com/s/MYePL0iNfGymOLcB2KVtug)，简单易懂的入门贴。
- [GNN图神经网络综述](https://luweikxy.gitbook.io/machine-learning-notes/graph-neural-networks/graph-neural-networks-review)，我勒个去，这个哥们整理的太全了，基本上可以覆盖所有的网上的资料了。
- [知乎上一个很赞的GNN系列七篇](https://zhuanlan.zhihu.com/p/185773047)

### 视频
- [PGL全球冠军团队带你攻破图神经网络](https://www.bilibili.com/video/BV1rf4y1v7cU/?spm_id_from=333.788.videocard.4)
- [图网络论文读书会 18 期分享：高飞《GNN可以有多强？》| 集智学园](https://www.bilibili.com/video/BV1c4411c7KM/?spm_id_from=333.788.videocard.8)
- [图神经网络在线研讨会2020.3.29后半场（转载）](https://www.bilibili.com/video/BV1w7411Q7pQ/?spm_id_from=333.788.videocard.11)
- [高级人工智能 第七讲-图神经网络 中国科学院大学 2020秋季 沈华伟](https://www.bilibili.com/video/BV1Ry4y1r73h?from=search&seid=6393305346667146785)
- [P23中科院自动化所常建龙：新生卷积神经网络—从欧几里得空间到非欧几里得空间1](https://www.bilibili.com/video/BV1j54y1975p?p=23)
- [P24MIT在读博士生金汶功：图表示学习在化学中的应用1:03:42](https://www.bilibili.com/video/BV1j54y1975p?p=24)
- [沈华伟：图神经网络的灵魂三问](https://www.bilibili.com/video/BV1Bv411k745/?spm_id_from=333.788.videocard.3)
- [图卷积神经网络-沈华伟](https://www.bilibili.com/video/BV1ta4y1t7EK/?spm_id_from=333.788.videocard.1)
- [图神经网络介绍-Introduction to Graph Neural Network（GNN）](https://www.bilibili.com/video/BV1At411N7nh/?spm_id_from=333.788.videocard.12)
- [台大李宏毅助教讲解GNN图神经网络](https://www.bilibili.com/video/BV1G54y1971S/?spm_id_from=333.788.videocard.0)
- [20201021 图神经网络：深图远算，理胜其辞](https://www.bilibili.com/video/BV1EV411y7V6?from=search&seid=6393305346667146785)
- [cs224w图神经网络中文简洁版graph model](https://www.bilibili.com/video/BV1jE411p7pj/?spm_id_from=333.788.videocard.0)
- [图深度表示（GNN）的基础和前沿进展](https://www.bilibili.com/video/BV1aJ411J7d5/?spm_id_from=333.788.b_636f6d6d656e74.5)
- [SFFAI分享 | 常建龙：基于关系的深度学习【附PPT】](https://www.bilibili.com/video/BV1Hx411X762/?spm_id_from=333.788.videocard.19)
- [从代码的角度深入浅出图神经网络（GNN）第一期](https://www.bilibili.com/video/BV1G54y1e7yh)
- [图神经网络在线研讨会](https://www.bilibili.com/video/BV1uV411f7Y8/?spm_id_from=333.788.videocard.30)
- [深度学习论文复现·NLP·】图神经网络【nodevec】CNN开山之作](https://www.bilibili.com/video/BV1fZ4y1N7yp/?spm_id_from=333.788.videocard.4)
- [中科院自动化所在读博士生高君宇：图神经网络在视频理解中的探索](https://www.bilibili.com/video/BV1Vb411W7t9/?spm_id_from=333.788.videocard.3)
- [北京邮电大学-石川教授-《异质信息网络的表示学习与应用》| SMP前沿技术讲习班](https://www.bilibili.com/video/BV1m441167LX/?spm_id_from=333.788.videocard.16)
- [图网络：融合推理与学习的全新深度学习架构——AI&Society 第八期-- 图神经网络及其它](https://campus.swarma.org/course/598/study)
- [基于图注意网络的链路预测](https://campus.swarma.org/course/1863)
- [非欧氏数据的几何深度学习](https://campus.swarma.org/course/240)
- [网络上的CNN-图卷积网络模型](https://campus.swarma.org/course/243)
- [利用图神经网络解决玻璃相变问题|DeepMind论文解读](https://campus.swarma.org/course/1371)
