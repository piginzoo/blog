---
layout: post
title: GNN图神经网络
category: machine-learning
---

## 概述

因为最近需要用到图卷积网络，所以需要对图神经网络做一个了解，之前也老听说GNN，GCN啥的，但是一直没有深入了解，借此机会，深入学习和理解了一下。

图神经网络涉及的内容非常多，我也不可能面面俱到，这篇文章也不打算做全面科普和讲解的目的，而是把自己的一些理解总结出来，至于很多在网上可以查到的资料，我只给出引用即可，不深入讲解，除非觉得有必要。

文章，主要是讲讲一下内容：
- 图、图神经网络的基本概念
- 图上如何做卷积和池化
- 卷积的两种：频谱卷积和空间卷积
- 常见的图卷积网络（注意不是图神经网络，那个范畴更大，我用不到，暂时也不做深入研究）
- 给出我收集的资料

### 图

先给定一个图的基本概念：

$G=(V,E,W)$

- V 是节点（vertex）集合，其中节点用一个d维向量表示，$n=\|V\|$
- X 是节点的向量矩阵，就列就是一个节点的表示，$X \in R^{n \times d}$
- E 是边（edge）集合
- W 是邻接矩阵（方阵），矩阵里的值，就是边的权重，$W \in R^{n \times n}$

可以把每个节点当做是一个信号（看，和信号处理勾稽上了）。

### 图神经网络的用途

好，有了图的基本概念，我们就会想，能拿拿图干嘛？

- 一个网络，我最终给这个网络一个分类
- 一个网络，我对网路中的每个节点进行分类（社交网络中的标签分类）
- 一个网络，我通过局部网络的结构（已知的标注网络）学习出另外部分的网络结构（如预测一个节点和别人的链接情况）（是否用来做OCR布局分析？）
- 还可以和GAN结合，用于生成一个网络结构（分子制药生成分子式）

你可以看出，其实主要焦点是在整体，还是个体上，整体就是整个大网的表示学习，个体就是每个节点的表示学习，我目前关注GNN，就是想把它应用到OCR的布局分析中，比如把bbox（识别的文字框）当做一个个的节点vertex，然后看他们彼此的关系如何？也就是有无链接？他们自己是什么分类？这样，我就能找出哪些bbox是标题，还是值，他们是否在一行，等等，这样的业务含义，从而满足商业需求。

## 图卷积神经网络

一个图，如何做特征抽取，我们自然而然就想象到过去我们最擅长的卷积神经网络（CNN）：

### 类比CNN

咱们学习CNN，先通过表示学习，学习出图的内在特征，然后用它做分类啥的。是的，没错。你为何会有这么一个思路，因为这个思路就是你过去CNN图像卷积神经网络的思路啊，把图像的特征抽取出来，然后做分类啥的。

![](/images/20201129/1606617898239.jpg){:class="myimg"}

好，我们看CNN，我们的两大法器就是卷积和池化，当然**核心是卷积**，那么这套思路是不是可以套到图神经网络上呢？

答案是，可以的，如下图，图上也可以做卷积完成特征抽取，然后做池化，缩小网络结构，当然，不是说的那么简单，这个后面我会逐一解释，这里先直观有个印象即可。

![](/images/20201129/1606620520337.jpg){:class="myimg"}

>这里，我自己有个疑问，也算是给自己挖个坑：在做卷积的时候，网络中变化的是什么？是每个节点上的表示么？表示的维度为变么？网络的结构会在变么？我知道池化肯定会变的，这种先不考虑呢？我说的变是整个结构都发生根本变化。这些问题，我提给自己，未来自己来把坑填上。

接下来说说卷积，

CNN只适合欧式空间，比如图像。而基于图（Graph）GNN更通用，适合非欧空间（啥叫欧式空间，就是支持[距离公式](https://baike.baidu.com/item/%E4%B8%A4%E7%82%B9%E9%97%B4%E8%B7%9D%E7%A6%BB%E5%85%AC%E5%BC%8F)的空间。）
可是，现在，我还是需要做卷积、池化，我改如何做？

现在问题变成了：图神经网络最核心的就是要解决卷积的问题，卷积最核心的就是卷积核如何确定。

### 先说说卷积

在讨论图（Graph）如何做卷积之前，我们有必要讨论一下，究竟什么是卷积？

本质上卷积核，是对在过滤不同频率的信号。[参考](https://zhuanlan.zhihu.com/p/28478034)。
你看卷积公式$h(t) = \int_{+\infty}^{-\infty} f(\tau)g(x-\tau) dt$，

![](/images/20201127/1606448534258.jpg)

[参考](https://www.bilibili.com/video/BV1Ry4y1r73h?from=search&seid=7657366210802615266)：视频22分钟处。

- 对应上图：$f(\tau)$就是右侧那个下降函数，$g(x-\tau)$是方波函数，而卷积结果（是个以时间$\t$为变量的函数）是左侧那个上升函数
- $h(t)$是一个以$t$为变量的函数，是的，他是个时间函数
- 设想某个$t=t_0$，$h(t_0)$就是那上图中黄色面积
- 这个面积是一个积分，是$\tau$从$-\infty ~ +\infty$的积分，上图上，就是黄色的面积

那么问题又来了，卷积有啥用？

其实，卷积本质是为了在频域空间的不同频率信号的过滤，原因是，时域卷积=频域相乘，这个是傅里叶变换的结论，细节我会在傅里叶变换的博文中详解，这里只知道结论就可以。你用一个滤波器函数$g(x)$去和你的目标函数$f(x)$做卷积，就相当于是把他们放到了频率里，进行相乘，相乘的结果，就相当于在频率里把某些频段的值给“乘”掉了（理解这个可以参考下面参考问斩各种的方波滤波器的概念），只剩下了你所需要的频段的信号了。

>时域卷积=频域相乘，卷积核本质上是一个二维函数，有对应的频谱函数，因而可以看成某种『滤波器』

[参考](https://zhuanlan.zhihu.com/p/28478034)

那么，把这个概念扩展到二维函数上，也就是图像上，图像可以看做是一个二维离散函数。
图像上的分类，其实就是图像上的信号处理。
那么我们来对应一下卷积的概念。
$f(x)和g(x)$的x，对应整个图像的二维位置（x，y），$f(x)$对应图像的值，$g(x)$就是对应的卷积核，时间$t$对应的就是卷积目标feature map上的一个点。你的卷积核，一个一个位置的挪动，就是在计算目标函数$h(t)$。看看，都对应上了吧！

可是，我们又要想，为何要做图像的卷积？

答案是，为了提取图像不同频率的信息。啥？！图像又不是波，怎么会有频率？有的！图像实际上可以看成是一个由不同的点上的值变化，所组成的一个东西。不同的点（二维），对应不同的值，这就是一个函数啊。既然是函数，就可以按照傅里叶变换，变换到频率里。这么说的太数学化，不好懂。你可以换个思路理解，就是像素间变化剧烈，比如从红色突然变成了白色，或者从255的值，突然变成了0，这种变化多剧烈啊！这就是高频啊。反之，就是低频啊。

### 图上的卷积

（*注：以下理解均来自这篇很赞的[图卷积的知乎解释贴](https://www.zhihu.com/question/54504471/answer/332657604)，感兴趣的可以参照阅读。*）

图片上可以做卷积，我们理解了，那么问题又来了？一个图（Graph）上，如何做卷积？

前面我们说了，卷积，其实就是一种滤波器，就是把图像变换到频域，然后用某个卷积核来过滤出某个频段的特性。

那么，我们同样的思路，把图上的信息，变换到频域里面，然后在里面做相乘，从而过滤出某个频段的信息。

还记得我们是如何把一个函数变换到频域里面么？用傅里叶变换。

![](/images/20201129/1606626872416.jpg){:class="myimg30"}




如何做图卷积的卷积核的参数共享？

### 图上的池化

CNN池化我们都很理解了，那么图上怎么池化呢？图像的池化是把几个点取平均池化或者max-pooling，也就是把几个点合成了一个点。在图像上，空间不变性，我们可以理解空间上的池化，很自然。可是到了图上，拓扑分布歪扭七八的，你咋个池化？咋个把多个node合而为一呢？

https://campus.swarma.org/course/598


### 分类


#### 谱方法

图上的拉普拉斯，

$L=D - W$

- W上面提到了，就是边上的权重，是一个对称方阵，$W \in R^{n \times n}$
- D呢？我看很多文章是都说图的节点的度的对角阵，但是实际上D的定义是：$D_{ij}= \sum_j W_{ij}$。

	这里不得不说一下，网上的狠多文章都是不考虑边权重的图，或者说，权重是1，但是如果是边带不同权重的，就需要用上述公式来确定矩阵D，即一个边出度上的边的权重之合。



#### 空间方法






## 典型GNN

### GCN

GCN可谓是图神经网络的“开山之作”，首次将图像卷积操作用到图结构。

$h_v = f(\frac{1}{\|N(v)\|} \sum_{u\in N(v)} Wx_u+b)  ,   \forall v \in V$

聚合邻居节点的特征然后做一个线性变换

https://www.zhihu.com/question/54504471

GCN的缺点:
- GCN需要将整个图放到内存和显存，这将非常耗内存和显存，处理不了大图；
- GCN在训练时需要知道整个图的结构信息（包括待预测的节点）

恩，你没看错，GCN每次都需要整个图，这没法玩啊。有的图太大，有的图还在“生长”中（或者很多未知部分），你咋玩？

### GraphSAGE

https://zhuanlan.zhihu.com/p/136521625

SAGE: Graph Sample and Aggregate.

GrachSAGE是为了解决GCN的上面说的两个问题：

>GCN的缺点:
- GCN需要将整个图放到内存和显存，这将非常耗内存和显存，处理不了大图；
- GCN在训练时需要知道整个图的结构信息（包括待预测的节点）


GCN输入了整个图，无法区分训练和验证集。而，GraphSAGE对k-hop（距离为k的节点们）进行采样，对这些采出来的节点上的embeding值进行聚合。

【咋采样？】

采用一个叫“定长抽样”的方法，定义需要的邻居个数 [公式] ，然后采用有放回的重采样/负采样方法达到 [公式] 。保证每个节点（采样后的）邻居个数一致？？？

【咋聚合？参数呢？】

就是如何根据周边的节点，算出当前目标节点的更新后的embeding值。(你看，其实图结构没变)

【损失函数】

如果是有监督的情况下，可以使用每个节点的预测lable和真实lable的交叉熵作为损失函数。如果是在无监督的情况下，可以假设相邻的节点的embedding表示尽可能相近。

【GraphSAGE得到了啥】

它是得到了每个node的embeding的向量，然后你可以单个取出一个向量来，去做分类、预测啥的了，每个node有个语义表示啦！

### GAT

![](/images/20201127/1606460968299.jpg){:class="myimg"}

## 实现

## 开源项目

## 参考

### 文档
- [知乎：如何理解 Graph Convolutional Network（GCN）？](https://www.zhihu.com/question/54504471/answer/332657604)，超赞的讨论帖
- [GNN 入门系列: 直观理解和应用介绍](https://mp.weixin.qq.com/s/MYePL0iNfGymOLcB2KVtug)，简单易懂的入门贴。
- [GNN图神经网络综述](https://luweikxy.gitbook.io/machine-learning-notes/graph-neural-networks/graph-neural-networks-review)，我勒个去，这个哥们整理的太全了，基本上可以覆盖所有的网上的资料了。
- [知乎上一个很赞的GNN系列七篇](https://zhuanlan.zhihu.com/p/185773047)
- [拉普拉斯矩阵与拉普拉斯算子的关系](https://zhuanlan.zhihu.com/p/85287578)

### 视频

【目前看过的】

- [图卷积神经网络-沈华伟](https://www.bilibili.com/video/BV1ta4y1t7EK/?spm_id_from=333.788.videocard.1)
- [高级人工智能 第七讲-图神经网络 中国科学院大学 2020秋季 沈华伟](https://www.bilibili.com/video/BV1Ry4y1r73h?from=search&seid=6393305346667146785)，沈老师讲的最最棒。
- [台大李宏毅助教讲解GNN图神经网络](https://www.bilibili.com/video/BV1G54y1971S/?spm_id_from=333.788.videocard.0)，宏毅老师的助教也确实不同凡响，讲的还不错。
- [图神经网络介绍-Introduction to Graph Neural Network（GNN）](https://www.bilibili.com/video/BV1At411N7nh/?spm_id_from=333.788.videocard.12)，不知名的台湾小哥的讲座，基本上也是GNN通识教育。
- [网络上的CNN-图卷积网络模型](https://campus.swarma.org/course/243)，集智的小姐姐的科普讲座，思路很清晰。
- [频域、谱域以及拉普拉斯算子在图上的应用](https://www.bilibili.com/video/BV164411x7L2?p=1)，小姐姐讲的脉络很清晰，可惜没讲完。
- [PGL全球冠军团队带你攻破图神经网络](https://www.bilibili.com/video/BV1rf4y1v7cU/?spm_id_from=333.788.videocard.4)
- [图网络论文读书会 18 期分享：高飞《GNN可以有多强？》 集智学园](https://www.bilibili.com/video/BV1c4411c7KM/?spm_id_from=333.788.videocard.8):这个是一个评价GNN框架的论文的讲解。

【待学习的】

- [图神经网络学习班-GCN-图神经网络认知及推理](https://www.bilibili.com/video/BV135411t76X?from=search&seid=6393305346667146785)
- [图神经网络在线研讨会2020.3.29后半场（转载）](https://www.bilibili.com/video/BV1w7411Q7pQ/?spm_id_from=333.788.videocard.11)
- [P23中科院自动化所常建龙：新生卷积神经网络—从欧几里得空间到非欧几里得空间1](https://www.bilibili.com/video/BV1j54y1975p?p=23)
- [P24MIT在读博士生金汶功：图表示学习在化学中的应用1:03:42](https://www.bilibili.com/video/BV1j54y1975p?p=24)
- [沈华伟：图神经网络的灵魂三问](https://www.bilibili.com/video/BV1Bv411k745/?spm_id_from=333.788.videocard.3)
- [20201021 图神经网络：深图远算，理胜其辞](https://www.bilibili.com/video/BV1EV411y7V6?from=search&seid=6393305346667146785)
- [cs224w图神经网络中文简洁版graph model](https://www.bilibili.com/video/BV1jE411p7pj/?spm_id_from=333.788.videocard.0)
- [图深度表示（GNN）的基础和前沿进展](https://www.bilibili.com/video/BV1aJ411J7d5/?spm_id_from=333.788.b_636f6d6d656e74.5)
- [SFFAI分享 常建龙：基于关系的深度学习【附PPT】](https://www.bilibili.com/video/BV1Hx411X762/?spm_id_from=333.788.videocard.19)
- [从代码的角度深入浅出图神经网络（GNN）第一期](https://www.bilibili.com/video/BV1G54y1e7yh)
- [图神经网络在线研讨会](https://www.bilibili.com/video/BV1uV411f7Y8/?spm_id_from=333.788.videocard.30)
- [深度学习论文复现·NLP·】图神经网络【nodevec】CNN开山之作](https://www.bilibili.com/video/BV1fZ4y1N7yp/?spm_id_from=333.788.videocard.4)
- [中科院自动化所在读博士生高君宇：图神经网络在视频理解中的探索](https://www.bilibili.com/video/BV1Vb411W7t9/?spm_id_from=333.788.videocard.3)
- [北京邮电大学-石川教授-《异质信息网络的表示学习与应用》 SMP前沿技术讲习班](https://www.bilibili.com/video/BV1m441167LX/?spm_id_from=333.788.videocard.16)
- [图网络：融合推理与学习的全新深度学习架构——AI&Society 第八期-- 图神经网络及其它](https://campus.swarma.org/course/598/study)
- [基于图注意网络的链路预测](https://campus.swarma.org/course/1863)
- [非欧氏数据的几何深度学习](https://campus.swarma.org/course/240)
- [利用图神经网络解决玻璃相变问题 DeepMind论文解读](https://campus.swarma.org/course/1371)
