---
layout: post
title: GNN图神经网络
category: machine-learning
---

## 概述

CNN只适合欧式空间，啥叫欧式空间，就是支持[距离公式](https://baike.baidu.com/item/%E4%B8%A4%E7%82%B9%E9%97%B4%E8%B7%9D%E7%A6%BB%E5%85%AC%E5%BC%8F)的空间。
GNN更通用，适合非欧空间。
那么问题来了，我欧式空间（如图像）上，可以做卷积，那我图上，如何做卷积呢？
图神经网络最核心的就是要解决卷积的问题。
卷积最核心的就是卷积核如何确定。
那么问题又来了，什么叫卷积的核？


### 图

先给定一个图的基本概念：

$G=(V,E,W)$

- V 是节点（vertex）集合，其中节点用一个d维向量表示，$n=\|V\|$
- X 是节点的向量矩阵，就列就是一个节点的表示，$X \in R^{n \times d}$
- E 是边（edge）集合
- W 是邻接矩阵（方阵），矩阵里的值，就是边的权重，$W \in R^{n \times n}$

可以把每个节点当做是一个信号（看，和信号处理勾稽上了）。

### 卷积

本质上卷积核，是对在过滤不同频率的信号。[参考](https://zhuanlan.zhihu.com/p/28478034)。
你看卷积公式$h(t) = \int_{+\infty}^{-\infty} f(\tau)g(x-\tau) dt$，

![](/images/20201127/1606448534258.jpg)

[参考](https://www.bilibili.com/video/BV1Ry4y1r73h?from=search&seid=7657366210802615266)：视频22分钟处。

- 对应上图：$f(\tau)$就是右侧那个下降函数，$g(x-\tau)$是方波函数，而卷积结果（是个以时间$\t$为变量的函数）是左侧那个上升函数
- $h(t)$是一个以$t$为变量的函数，是的，他是个时间函数
- 设想某个$t=t_0$，$h(t_0)$就是那上图中黄色面积
- 这个面积是一个积分，是$\tau$从$-\infty ~ +\infty$的积分，上图上，就是黄色的面积

那么问题又来了，卷积有啥用？

其实，卷积本质是为了在频域空间的不同频率信号的过滤，原因是，时域卷积=频域相乘，这个是傅里叶变换的结论，细节我会在傅里叶变换的博文中详解，这里只知道结论就可以。你用一个滤波器函数$g(x)$去和你的目标函数$f(x)$做卷积，就相当于是把他们放到了频率里，进行相乘，相乘的结果，就相当于在频率里把某些频段的值给“乘”掉了（理解这个可以参考下面参考问斩各种的方波滤波器的概念），只剩下了你所需要的频段的信号了。

>时域卷积=频域相乘，卷积核本质上是一个二维函数，有对应的频谱函数，因而可以看成某种『滤波器』

[参考](https://zhuanlan.zhihu.com/p/28478034)

那么，把这个概念扩展到二维函数上，也就是图像上，图像可以看做是一个二维离散函数。
图像上的分类，其实就是图像上的信号处理。
那么我们来对应一下卷积的概念。
$f(x)和g(x)$的x，对应整个图像的二维位置（x，y），$f(x)$对应图像的值，$g(x)$就是对应的卷积核，时间$t$对应的就是卷积目标feature map上的一个点。你的卷积核，一个一个位置的挪动，就是在计算目标函数$h(t)$。看看，都对应上了吧！

可是，我们又要想，为何要做图像的卷积？

答案是，为了提取图像不同频率的信息。啥？！图像又不是波，怎么会有频率？有的！图像实际上可以看成是一个由不同的点上的值变化，所组成的一个东西。不同的点（二维），对应不同的值，这就是一个函数啊。既然是函数，就可以按照傅里叶变换，变换到频率里。这么说的太数学化，不好懂。你可以换个思路理解，就是像素间变化剧烈，比如从红色突然变成了白色，或者从255的值，突然变成了0，这种变化多剧烈啊！这就是高频啊。反之，就是低频啊。

### 图上的卷积

图片上可以做卷积，我们理解了，那么问题又来了？一个图（Graph）上，如何做卷积？


如何做图卷积的卷积核的参数共享？

### 分类


#### 谱方法

图上的拉普拉斯，

$L=D - W$

- W上面提到了，就是边上的权重，是一个对称方阵，$W \in R^{n \times n}$
- D呢？我看很多文章是都说图的节点的度的对角阵，但是实际上D的定义是：$D_{ij}= \sum_j W_{ij}$。

	这里不得不说一下，网上的狠多文章都是不考虑边权重的图，或者说，权重是1，但是如果是边带不同权重的，就需要用上述公式来确定矩阵D，即一个边出度上的边的权重之合。

#### 空间方法






## 典型GNN

### GCN

GCN可谓是图神经网络的“开山之作”，首次将图像卷积操作用到图结构。

$h_v = f(\frac{1}{\|N(v)\|} \sum_{u\in N(v)} Wx_u+b)  ,   \forall v \in V$

聚合邻居节点的特征然后做一个线性变换

https://www.zhihu.com/question/54504471

GCN的缺点:
- GCN需要将整个图放到内存和显存，这将非常耗内存和显存，处理不了大图；
- GCN在训练时需要知道整个图的结构信息(包括待预测的节点)

### GraphSAGE

https://zhuanlan.zhihu.com/p/136521625

Graph Sample and Aggregate

GCN输入了整个图，无法区分训练和验证集。

GraphSAGE对k-hop（距离为k的节点们）进行采样，对这些采出来的节点上的embeding值进行聚合，

【咋采样？】

采用一个叫“定长抽样”的方法，定义需要的邻居个数 [公式] ，然后采用有放回的重采样/负采样方法达到 [公式] 。保证每个节点（采样后的）邻居个数一致？？？

【咋聚合？参数呢？】

就是如何根据周边的节点，算出当前目标节点的更新后的embeding值。(你看，其实图结构没变)

【损失函数】

如果是有监督的情况下，可以使用每个节点的预测lable和真实lable的交叉熵作为损失函数。如果是在无监督的情况下，可以假设相邻的节点的embedding表示尽可能相近。


### GAT

![](/images/20201127/1606460968299.jpg){:class="myimg"}

## 实现

## 开源项目

## 参考

### 文档
- [GNN 入门系列: 直观理解和应用介绍](https://mp.weixin.qq.com/s/MYePL0iNfGymOLcB2KVtug)，简单易懂的入门贴。
- [GNN图神经网络综述](https://luweikxy.gitbook.io/machine-learning-notes/graph-neural-networks/graph-neural-networks-review)，我勒个去，这个哥们整理的太全了，基本上可以覆盖所有的网上的资料了。
- [知乎上一个很赞的GNN系列七篇](https://zhuanlan.zhihu.com/p/185773047)

### 视频
- [李宏毅老师的助教姜成汉的GNN分享](https://www.bilibili.com/video/BV1G54y1971S)
- [国防科技大学沈华伟老师的GNN课程](https://www.bilibili.com/video/BV1Ry4y1r73h)
- [台湾小哥杜岳华的GNN分享](https://www.bilibili.com/video/BV1At411N7nh?t=4425)
- [手撸了GNN](https://space.bilibili.com/630192628)，一个小哥们系列课程，手撸GNN
