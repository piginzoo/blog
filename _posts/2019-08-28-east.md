---
layout: post
title: EAST的理解及其代码实现
category: machine-learning
---

# 概述

网上文章，很多了，他们讲的我都不想细讲了，可以参考最后的参考文档，我只会谈谈我的一些理解。

是的，奇怪的废话少说，直奔主题吧....

# 论文解读

先给个下载地址：<https://arxiv.org/pdf/1704.03155.pdf>

说啥pipeline，说白了就是2步就文本检测这事儿搞定，要是你搞过CTPN，就知道丫多繁琐。这两步是啥？
- 一个网络，砰地一声，就检测出每个点是否是文本点，以及他到围绕他的那个框的4个距离
- 再来一个local aware NMS
完事！

![](/images/20190828/1566983244013.jpg){:class="myimg"}

这图很多人都讲过了，他们见过的东东，我就不提了，补充我自己的理解：

- argman大神的代码子用的是resnet V50，无所谓啦，只要这个pretrain网络可以吐出来conv4,conv3,conv2,conv1 4个feature map，而且是每个都是上一个的1/2就成
- 从最后uppooling+contact后的合成的最后的那个大图中，实际上是512x512x32的那个feature map，到最后的score map/geometry map/angel map，就是绿色=>蓝色的这块是咋做的呢？答案是，用的是一个1x1的卷积实现的
```python
F_score = slim.conv2d(g[3],1,1, activation_fn=tf.nn.sigmoid, normalizer_fn=None)
geo_map = slim.conv2d(g[3], 4, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) * FLAGS.text_scale
angle_map = (slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) - 0.5) * np.pi/2 # angle is between [-45, 45]
```
- 

![](/images/20190828/1566983699868.jpg){:class="myimg"}

再说说这图，这图是啥啊？是告诉你样本咋做。
- 左上那个黄框是GT（标注），但是真正用的正样本，是绿框，往里缩进了1/3，你问为何，我也不知道，我估计是让这些点更靠里，更可信吧。而且，这样预测的4个值（上下左右）可能才都大小差不多。
- 最后我们的样本是啥，是5张图，对，6张图，跟输入的数据，原图，大小一样，都是1通道的
- b就是一个score map的例子，很直观，但是geometry map就不一样了，每个像素点代表的值是啥呢？是他们到对应位置的长度。啥意思？比如geometry map中的第一张，就是每个像素，到他对应的那个文本框的上边的距离，对，这个geometry map图像中的每个值，都是这个像素距离他对应的文本框上沿的距离。所以，geometry图，4张，分别应该叫“上沿图、下沿图、左沿图、右沿图”，这个理解了，角度图也应该理解。合计，你算算，正好是6张图。



# 代码解读

![](/images/20190828/1566987583219.jpg){:class="myimg30"}

# 参考

- [七月在线有个不错的OCR课程](https://www.julyedu.com/course/getDetail/138/),里面的好未来的杨老师讲了EAST，不错
- Argman大神的[Github repo](https://github.com/argman/EAST)，我们的代码就是fork于他
- <https://cloud.tencent.com/developer/article/1329171>
- <https://zhuanlan.zhihu.com/p/37504120>

