---
title: 机器学习 & 深度学习 [笔记]
date: 2017-3-27T04:07:56+00:00
author: 动物园的猪
layout: post
categories:
  - 机器学习
---
把自己的一些学习笔记都写到这上面，记录下来方便复习和查找


EM
===
EM知道分类数量么?
这篇讲的很好:
http://blog.csdn.net/u012990623/article/details/42323661
为了解决这个你依赖我，我依赖你的循环依赖问题，总得有一方要先打破僵局，说，不管了，我先随便整一个值出来，看你怎么变，然后我再根据你的变化调整我的变化，然后如此迭代着不断互相推导，最终就会收敛到一个解。这就是EM算法的基本思想了。




贝叶斯
======
灵活掌握贝叶斯公式,
贝叶斯网络:是个有向图,两个节点产生一个条件概率,节点是随机变量
p(a,b,c)=p(c|a,b)*p(b|a)*p(a)
a----->b---->c
   \------------^
P(b|a)  ===>  a<----b
HMM,
比较诡异的是,x1,x2,...xn是不独立的!!!之前的回归,样本都是独立的!
比如语言有上下文,所以彼此词词间是不独立的.
共线
I(A,B)互信息 = 
最小生成树,kruskal算法
这篇是GAN必看的论文之一

贝叶斯实践
======
GaussianNB, 二项分布的NB,...的NB,各种分布都有NB的,他们之间是可以组合的.
NB--->朴素贝叶斯 Naive Beyes?
朴素贝叶斯的前提是,1:各个特征之间是条件独立的(条件独立性),特别是适合自然语言处理,2:特征同等重要
有时候不是条件独立,但是我们人为地让他独立,但是可以解决问题就可以.
"特征之间的共线性"? 老师提到的概念
文档单词组成的0/1向量是稀疏矩阵,但是维度一样.
如果一个词总在某个文档中出现,在其他文档中很少出现,那么这个文档中,这个词重要性就高,
逆文档频率
在确定概率的情况下，有没有蒙特卡洛模拟得出的结论是一样的？
蒙特卡洛,应用在树的搜索,不是用于建模的方法.
word2vec
K近邻 knightbour?

LDA
======
beta分布, theta的值是(0,1),
共轭是没有数学基础的,是为了工程简单的
LDA和语言无关的,
50维是指的50个主题么? 主题咋来de?
高频词和LDA主题词关系和区别?
选择共轭的原因是,是可以循环递归来回做,直到收敛到某个值.
噢,dirichlet
机器学习得到主题，而主题的实际意义需要等后续业务人员来归纳分析吧

语料库:
http://mp.weixin.qq.com/s/0DkookBoiM4Db0VcbVRZhg

变分实现的Gensim开源的LDA,
LDA实现起来有点复杂,

HMM
======
用HMM做分词,甚至不需要词库,神奇...
马尔科夫链实际上是一个只考虑前一个状态的条件概率,在之前的都不考虑了,这样做只是为了简化模型
讲的最好的还是吴军的数学之美
网上的盒子+红黑球是个简单易懂的例子,邹博也用它:https://www.zhihu.com/question/20962240
HMM就是在马尔科夫上加了一个输出层(观察层),之前的状态变成了隐含层,
状态层的各个状态的转移矩阵是一个概率,一般状态机迁移可以用矩阵表示,邹博讲过
而那个所谓的"混淆矩阵"是隐含层状态<->输出层状态的转移概率.
pi是开始时候的状态.
邹博的两个例子很赞:一个是中文分词,一个是红黑球,不会了就回去听听
HMM有3种玩法,就


SVM
====
Kernel函数, 正定矩阵相乘,
各个点的点乘函数,



提升
====
提到了梯度下降的时候,步长值是可以通过回溯线性搜索试探出来的,
是他之前的"梯度下降与牛顿"的22/56页提到的,叫Armijo准则,

 


决策树+随机森林
=============
DecisionTreeRegression可以做回归么?就是预测连续值
邹博的13提升课开头的例子12.4貌似就是,
m
随机森林比决策树要平滑,不是锯齿状那么明显
随机森林可以得到一个强分类器?





